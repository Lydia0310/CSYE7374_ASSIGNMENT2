{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS,SVMWithSGD,LogisticRegressionWithSGD, NaiveBayes\n",
    "from pyspark.mllib.regression import LabeledPoint,LinearRegressionWithSGD,RidgeRegressionWithSGD,LassoWithSGD\n",
    "from pyspark.mllib.tree import DecisionTree, RandomForest, GradientBoostedTrees\n",
    "from pyspark.mllib.feature import StandardScaler,ChiSqSelector,Normalizer,PCA\n",
    "from pyspark.mllib.evaluation import RegressionMetrics, BinaryClassificationMetrics\n",
    "from pyspark.mllib.stat import Statistics, MultivariateStatisticalSummary\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.regression import DecisionTreeRegressor, LinearRegression, RandomForestRegressor\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, RegressionEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def importRawData(sc, filePath1,filePath2):\n",
    "    \"\"\"\n",
    "    :param sc: Spark Context\n",
    "    :param filePath: path to data .csv file\n",
    "    :return: RDD of (LabeledPoint, index)\n",
    "    \"\"\"\n",
    "    rdd1 = sc.textFile(filePath1)\n",
    "    rdd2 = sc.textFile(filePath2)\n",
    "    # index was keep for this dataset, we need index to perform partition\n",
    "    # spark DOES NOT gurantee tranformation and action execute order for RDD[Vector]\n",
    "    Trdd = rdd1.map(lambda line: line.split(\",\")) \\\n",
    "                 .map(lambda array: [float(n) for n in array])\n",
    "    Vrdd = rdd2.map(lambda line: line.split(\",\")) \\\n",
    "                 .map(lambda array: [float(n) for n in array]) \n",
    "            \n",
    "    return (Trdd,Vrdd)\n",
    "\n",
    "\n",
    "\n",
    "def featureEngineering(trainingData,validationData, corrSelection = None, zNorm = True, l2Norm = True, categorical = True, topFeature = 10):\n",
    "    \"\"\"\n",
    "    this function is to provide feature engineering and transformation\n",
    "    including partition, normalization, feature selection, dimension reduction\n",
    "    :param rawData: RDD[Vector] of raw data\n",
    "    :param corrSelection: local matrix of correliation matrix, if provided will perform CFS\n",
    "    :param zNorm: boolean of whether to perform Z normalization or not\n",
    "    :param l2Norm: boolean of whether to perform L2 normalization or not\n",
    "    :param categorical: boolean of whether to perform chi square feature selection or not\n",
    "    :param topFeature: select top features if using chi square selection\n",
    "    :return: tuple of (RDD[LabeledPoint] trainingData, RDD[LabeledPoint] validationData)\n",
    "    \"\"\"\n",
    "    print \"==============================================\"\n",
    "    print \"FEATURE ENGINEERING\"\n",
    "    print \"==============================================\"\n",
    "    print \"partitioning...\"\n",
    "    # beware this partitioning is HARD CODED!\n",
    "    # because it was suggested by data set creator\n",
    "    # first 463715 used as trainning, last 51630 used as validation\n",
    "    tFeatures = trainingData.map(lambda a:a[:-1])\n",
    "    vFeatures = validationData.map(lambda a:a[:-1])\n",
    "    tLabel = trainingData.map(lambda a:a[-1])\n",
    "    vLabel = validationData.map(lambda a:a[-1])\n",
    "    \n",
    "    print \"Normalization and Scaling... \"\n",
    "    if(zNorm):\n",
    "        zN = StandardScaler(withMean=True, withStd=True).fit(tFeatures)\n",
    "        tFeatures = zN.transform(tFeatures)\n",
    "        vFeatures = zN.transform(vFeatures)\n",
    "    if(l2Norm):\n",
    "        l2N = Normalizer()\n",
    "        tFeatures = l2N.transform(tFeatures)\n",
    "        vFeatures = l2N.transform(vFeatures)\n",
    "    '''\n",
    "    print \"Feature Selection... \"\n",
    "    \n",
    "    # only categorical value for classification problem could use chi square selector\n",
    "    # otherwise, use correlation based selector instead\n",
    "    if categorical or (corrSelection is None):\n",
    "        \n",
    "        selector = ChiSqSelector(topFeature).fit(tLabel.zip(tFeatures).map(lambda lp:LabeledPoint(lp[0], lp[1])))\n",
    "        tFeatures = selector.transform(tFeatures)\n",
    "        vFeatures = selector.transform(vFeatures)\n",
    "    else:\n",
    "        bestFeatures = cfs(corrSelection)\n",
    "        tFeatures = tFeatures.map(lambda a:[a[n-1] for n in bestFeatures])\n",
    "        vFeatures = vFeatures.map(lambda a:[a[n-1] for n in bestFeatures])\n",
    "    featureCount = len(tFeatures.first())\n",
    "    print \"Selected %s Features: \" % featureCount\n",
    "    print \"Dimension Reduction...\"\n",
    "    # PCA dimension reduction = EVD of cov matrix = SVD of data matrix\n",
    "    # spark use EVD implement, hence we need to calculate eigen vector ourself to decide cutoff\n",
    "    # cutoff point the reduced dimension could represent 90% of original total variance\n",
    "    selector = PCA(len(bestFeatures)).fit(tFeatures.map(lambda a: LabeledPoint(1,a).features))\n",
    "    temp = selector.transform(tFeatures)\n",
    "    eigen = eigenvalues(temp)\n",
    "    print \"PCA Eigen Vector: %s\" % eigen\n",
    "    accVarPor = [sum(eigen[:i+1])/sum(eigen) for i in range(len(eigen))]\n",
    "    print \"Accumulate Variance Porpotion:\"\n",
    "    reduction = 0\n",
    "    for i in range(len(accVarPor)):\n",
    "        print \"%s Dimensions:  %.2f%%\" % (i+1, accVarPor[i]*100)\n",
    "        if(reduction==0 and accVarPor[i]>0.9): reduction = i+1\n",
    "    print \"Reduce to %s dimensions...\" % reduction\n",
    "    selector = PCA(reduction).fit(tFeatures.map(lambda a: LabeledPoint(1,a).features))\n",
    "    '''\n",
    "    return (tLabel.zip(tFeatures).map(lambda lp:LabeledPoint(lp[0], lp[1])).cache(), \\\n",
    "            vLabel.zip(vFeatures).map(lambda lp:LabeledPoint(lp[0], lp[1])).cache())\n",
    "'''\n",
    "def eigenvalues(data):\n",
    "    \"\"\"\n",
    "    helper function of calculate eigen vector of PCA, to determine how many dimension after reduction\n",
    "    :param data: RDD[Vector] of data after PCA transformation\n",
    "    :return: local list of eigen vector based on input data\n",
    "    \"\"\"\n",
    "    # only eigen value is calculated, not the whole covariance because that would be too slow\n",
    "    count = data.count()\n",
    "    miu = data.reduce(lambda a,b:[a[i]+b[i] for i in range(len(a))])\n",
    "    miu = map(lambda s:s/count, miu)\n",
    "    eigen = data.map(lambda a:[(a[i]-miu[i])**2 for i in range(len(a))]) \\\n",
    "                .reduce(lambda a,b:[a[i]+b[i] for i in range(len(a))])\n",
    "    eigen = map(lambda s:s/count, eigen)\n",
    "    return eigen\n",
    "'''\n",
    "def selectClassificationModel(sc, trainingData, validationData):\n",
    "    \"\"\"\n",
    "    wrapper function to evaluate and select all the classification models\n",
    "    :param sc: spark context\n",
    "    :param trainingData: RDD[LabeledPoint] of training data\n",
    "    :param validationData: RDD[LabeledPoint] of validation data\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    print \"==============================================\"\n",
    "    print \"CLASSIFICATION\"\n",
    "    print \"==============================================\"\n",
    "    classificationModels = [\n",
    "                                (SVMWithSGD, {\"intercept\":True, \"regType\":None}), \n",
    "                                (SVMWithSGD, {\"intercept\":True, \"regType\":\"l1\"}), \n",
    "                                (SVMWithSGD, {\"intercept\":True, \"regType\":\"l2\"}), \n",
    "                                (LogisticRegressionWithLBFGS, {\"intercept\":True, \"regType\":\"l1\"}), \n",
    "                                (LogisticRegressionWithLBFGS, {\"intercept\":True, \"regType\":\"l2\"}), \n",
    "                                (LogisticRegressionWithSGD, {\"intercept\":True, \"regType\":None}), \n",
    "                                (LogisticRegressionWithSGD, {\"intercept\":True, \"regType\":\"l1\"}), \n",
    "                                (LogisticRegressionWithSGD, {\"intercept\":True, \"regType\":\"l2\"})\n",
    "                           ]\n",
    "    for modelClass, kwargs in classificationModels:\n",
    "        trainClassificationModel(sc, trainingData, validationData, modelClass, **kwargs)\n",
    "    #GBT is waaaaaay too slow for this dataset\n",
    "    classificationModels = [\n",
    "                                (DecisionTree, {\"numClasses\":2,\"categoricalFeaturesInfo\":{},\"minInstancesPerNode\":100 ,\"impurity\":\"gini\"}), \n",
    "                                (DecisionTree, {\"numClasses\":2,\"categoricalFeaturesInfo\":{},\"minInstancesPerNode\":100 , \"impurity\":\"entropy\"}), \n",
    "                                (RandomForest, {\"numClasses\":2,\"categoricalFeaturesInfo\":{},\"numTrees\":20, \"impurity\":\"gini\"}), \n",
    "                                (RandomForest, {\"numClasses\":2,\"categoricalFeaturesInfo\":{},\"numTrees\":20, \"impurity\":\"entropy\"}) \n",
    "                           ]\n",
    "    for modelClass, kwargs in classificationModels:\n",
    "        trainClassificationTreeModel(sc, trainingData, validationData, modelClass, **kwargs)\n",
    "\n",
    "def trainClassificationModel(sc, trainingData, validationData, modelClass, **kwargs):\n",
    "    \"\"\"\n",
    "    train classification models for NOT-TREE based model\n",
    "    :param sc: spark context\n",
    "    :param trainingData: RDD[LabeledPoint] of training data\n",
    "    :param validationData: RDD[LabeledPoint] of validation data\n",
    "    :modelClass: model CLASS that use to train\n",
    "    :kwargs: key-value paired arguments for modelClass, would be passes in directly\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    print \"Classification Model: %s %s\" % (modelClass.__name__, kwargs)\n",
    "    startTime = time.time()\n",
    "    trainingData = trainingData \\\n",
    "                    .map(lambda lp:LabeledPoint(1, lp.features) if lp.label>=2000 else LabeledPoint(0, lp.features))\n",
    "    validationData = validationData \\\n",
    "                    .map(lambda lp:LabeledPoint(1, lp.features) if lp.label>=2000 else LabeledPoint(0, lp.features))\n",
    "    model = modelClass.train(trainingData, **kwargs)\n",
    "    model.clearThreshold()\n",
    "    validationsResult = validationData.map(lambda lp:(float(model.predict(lp.features)), lp.label))\n",
    "    metric = BinaryClassificationMetrics(validationsResult)\n",
    "    # the error rate search is to search for overall best error rate\n",
    "    # regardless of precision and recall, however they could be evaluate by PR area and ROC area\n",
    "    errors = []\n",
    "    for i in range(1, 11):\n",
    "        err = validationsResult.filter(lambda (predict,label):(1 if predict>i/10.0 else 0)!=label).count() \\\n",
    "                                            / float(validationsResult.count())\n",
    "        errors.append((err, i/10.0))\n",
    "    errors.sort(key=lambda t:t[0])\n",
    "    print \"[ Error: %.4f\\t\\tPrecision-recall: %.4f\\tROC: %.4f ] - %s sec\" \\\n",
    "            % (errors[0][0], metric.areaUnderPR, metric.areaUnderROC, (time.time()-startTime))\n",
    "        \n",
    "def trainClassificationTreeModel(sc, trainingData, validationData, modelClass, **kwargs):\n",
    "    \"\"\"\n",
    "    train classification models for TREE based model\n",
    "    :param sc: spark context\n",
    "    :param trainingData: RDD[LabeledPoint] of training data\n",
    "    :param validationData: RDD[LabeledPoint] of validation data\n",
    "    :modelClass: model CLASS that use to train\n",
    "    :kwargs: key-value paired arguments for modelClass, would be passes in directly\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    print \"Classification Model: %s %s\" % (modelClass.__name__, kwargs)\n",
    "    startTime = time.time()\n",
    "    trainingData = trainingData \\\n",
    "                    .map(lambda lp:LabeledPoint(1, lp.features) if lp.label>=2000 else LabeledPoint(0, lp.features))\n",
    "    validationData = validationData \\\n",
    "                    .map(lambda lp:LabeledPoint(1, lp.features) if lp.label>=2000 else LabeledPoint(0, lp.features))\n",
    "    model = modelClass.trainClassifier(trainingData, **kwargs)\n",
    "    validationFeatures = validationData.map(lambda lp:lp.features)\n",
    "    # !!!beware, due to some stange bug, DO NOT chain RDD transformation on tree model predict, count() immediately!!!\n",
    "    validationsResult = model.predict(validationFeatures)\n",
    "    totalCount = validationsResult.count()\n",
    "    validationsResult = validationsResult.zip(validationData.map(lambda lp:lp.label))\n",
    "    errCount = validationsResult.filter(lambda (predict,label):predict!=label).count()\n",
    "    validationsResult = validationsResult.zip(validationData.map(lambda lp:lp.label))\n",
    "    err = float(errCount) / totalCount\n",
    "    print \"[ Error: %.4f ] - %s sec\" % (err, (time.time()-startTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def selectClassificationPipelineModel(sqlContext, trainingData, validationData):\n",
    "    \"\"\"\n",
    "    wrapper function to evaluate and select all the classification models using pipeline CV\n",
    "    :param sqlContext: spark sql context\n",
    "    :param trainingData: RDD[LabeledPoint] of training data\n",
    "    :param validationData: RDD[LabeledPoint] of validation data\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    print \"==============================================\"\n",
    "    print \"CLASSIFICATION WITH PIPELINE CV\"\n",
    "    print \"==============================================\"\n",
    "    classificationModels = [\n",
    "                                (LogisticRegression(), {\"fitIntercept\":True, \"regParam\":[0.1,0.01,0.001]})\n",
    "                           ]\n",
    "    for modelObject, kwargs in classificationModels:\n",
    "        trainClassificationPipelineModel(sqlContext, trainingData, validationData, modelObject, kwargs)\n",
    "    \n",
    "def trainClassificationPipelineModel(sqlContext, trainingData, validationData, modelObject, kwargs):\n",
    "    \"\"\"\n",
    "    train classification models for NOT-TREE based model\n",
    "    :param sqlContext: spark sql context\n",
    "    :param trainingData: RDD[LabeledPoint] of training data\n",
    "    :param validationData: RDD[LabeledPoint] of validation data\n",
    "    :modelObject: model OBJECT that use to train\n",
    "    :kwargs: key-value dict for cross validation, would be passes in directly\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    print \"Classification Model: %s %s\" % (modelObject.__class__.__name__, kwargs)\n",
    "    startTime = time.time()\n",
    "    trainingData = trainingData \\\n",
    "                    .map(lambda lp:LabeledPoint(1, lp.features) if lp.label>=2000 else LabeledPoint(0, lp.features))\n",
    "    validationData = validationData \\\n",
    "                    .map(lambda lp:LabeledPoint(1, lp.features) if lp.label>=2000 else LabeledPoint(0, lp.features))\n",
    "    trainingDF = sqlContext.createDataFrame(trainingData).cache()\n",
    "    validationDF = sqlContext.createDataFrame(validationData).cache()\n",
    "    grid = ParamGridBuilder().baseOn(kwargs).build()\n",
    "    evaluator = BinaryClassificationEvaluator()\n",
    "    cv = CrossValidator(estimator=modelObject, estimatorParamMaps=grid, evaluator=evaluator, numFolds = 5)\n",
    "    cvModel = cv.fit(trainingDF)\n",
    "    validationResult = cvModel.transform(validationDF)\n",
    "    areaUnderROC = evaluator.evaluate(validationResult, {evaluator.metricName: \"areaUnderROC\"})\n",
    "    areaUnderPR = evaluator.evaluate(validationResult, {evaluator.metricName: \"areaUnderPR\"})\n",
    "    print \"[ Precision-recall: %.4f\\tROC: %.4f ] - %s sec\" \\\n",
    "            % (areaUnderPR, areaUnderROC, (time.time()-startTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================\n",
      "FEATURE ENGINEERING\n",
      "==============================================\n",
      "partitioning...\n",
      "Normalization and Scaling... \n",
      "==============================================\n",
      "CLASSIFICATION\n",
      "==============================================\n",
      "Classification Model: SVMWithSGD {'regType': None, 'intercept': True}\n",
      "[ Error: 0.0000\t\tPrecision-recall: 0.0000\tROC: 0.0000 ] - 15.1522059441 sec\n",
      "Classification Model: SVMWithSGD {'regType': 'l1', 'intercept': True}\n",
      "[ Error: 0.0000\t\tPrecision-recall: 0.0000\tROC: 0.0000 ] - 13.6986620426 sec\n",
      "Classification Model: SVMWithSGD {'regType': 'l2', 'intercept': True}\n",
      "[ Error: 0.0000\t\tPrecision-recall: 0.0000\tROC: 0.0000 ] - 9.83493089676 sec\n",
      "Classification Model: LogisticRegressionWithLBFGS {'regType': 'l1', 'intercept': True}\n",
      "[ Error: 0.0000\t\tPrecision-recall: 0.0000\tROC: 0.0000 ] - 7.79879188538 sec\n",
      "Classification Model: LogisticRegressionWithLBFGS {'regType': 'l2', 'intercept': True}\n",
      "[ Error: 0.0000\t\tPrecision-recall: 0.0000\tROC: 0.0000 ] - 6.30630803108 sec\n",
      "Classification Model: LogisticRegressionWithSGD {'regType': None, 'intercept': True}\n",
      "[ Error: 0.0000\t\tPrecision-recall: 0.0000\tROC: 0.0000 ] - 9.68229293823 sec\n",
      "Classification Model: LogisticRegressionWithSGD {'regType': 'l1', 'intercept': True}\n",
      "[ Error: 0.0000\t\tPrecision-recall: 0.0000\tROC: 0.0000 ] - 9.14455604553 sec\n",
      "Classification Model: LogisticRegressionWithSGD {'regType': 'l2', 'intercept': True}\n",
      "[ Error: 0.0000\t\tPrecision-recall: 0.0000\tROC: 0.0000 ] - 9.29331111908 sec\n",
      "Classification Model: DecisionTree {'categoricalFeaturesInfo': {}, 'minInstancesPerNode': 100, 'numClasses': 2, 'impurity': 'gini'}\n",
      "[ Error: 0.0000 ] - 1.9360640049 sec\n",
      "Classification Model: DecisionTree {'categoricalFeaturesInfo': {}, 'minInstancesPerNode': 100, 'numClasses': 2, 'impurity': 'entropy'}\n",
      "[ Error: 0.0000 ] - 1.39841508865 sec\n",
      "Classification Model: RandomForest {'categoricalFeaturesInfo': {}, 'numTrees': 20, 'impurity': 'gini', 'numClasses': 2}\n",
      "[ Error: 0.0000 ] - 1.69109106064 sec\n",
      "Classification Model: RandomForest {'categoricalFeaturesInfo': {}, 'numTrees': 20, 'impurity': 'entropy', 'numClasses': 2}\n",
      "[ Error: 0.0000 ] - 1.66145896912 sec\n",
      "==============================================\n",
      "CLASSIFICATION WITH PIPELINE CV\n",
      "==============================================\n",
      "Classification Model: LogisticRegression {'fitIntercept': True, 'regParam': [0.1, 0.01, 0.001]}\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o2540.fit.\n: java.lang.ArrayIndexOutOfBoundsException: 1\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:183)\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:52)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:90)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:71)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:207)\n\tat java.lang.Thread.run(Thread.java:744)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-42536741dcc6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mselectClassificationModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainingData\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidationData\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;31m#selectRegressionModel(sc, trainingData, validationData)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mselectClassificationPipelineModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msqlContext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainingData\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidationData\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[1;31m#selectRegressionPipelineModel(sqlContext, trainingData, validationData)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-33-395d84a534a2>\u001b[0m in \u001b[0;36mselectClassificationPipelineModel\u001b[1;34m(sqlContext, trainingData, validationData)\u001b[0m\n\u001b[0;32m     14\u001b[0m                            ]\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mmodelObject\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mclassificationModels\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mtrainClassificationPipelineModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msqlContext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainingData\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidationData\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodelObject\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtrainClassificationPipelineModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msqlContext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainingData\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidationData\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodelObject\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-33-395d84a534a2>\u001b[0m in \u001b[0;36mtrainClassificationPipelineModel\u001b[1;34m(sqlContext, trainingData, validationData, modelObject, kwargs)\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[0mevaluator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBinaryClassificationEvaluator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[0mcv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCrossValidator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodelObject\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mestimatorParamMaps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgrid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevaluator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumFolds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m     \u001b[0mcvModel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainingDF\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m     \u001b[0mvalidationResult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcvModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidationDF\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[0mareaUnderROC\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidationResult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mevaluator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetricName\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"areaUnderROC\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/spark/python/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m     63\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[1;32m/usr/local/spark/python/pyspark/ml/tuning.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    220\u001b[0m             \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mcondition\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumModels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 222\u001b[1;33m                 \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepm\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    223\u001b[0m                 \u001b[1;31m# TODO: duplicate evaluator to take extra params from input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m                 \u001b[0mmetric\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meva\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepm\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/spark/python/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[0mjava_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    126\u001b[0m         \"\"\"\n\u001b[0;32m    127\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/py4j/java_gateway.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    536\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m         return_value = get_return_value(answer, self.gateway_client,\n\u001b[1;32m--> 538\u001b[1;33m                 self.target_id, self.name)\n\u001b[0m\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/py4j/protocol.pyc\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    298\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    299\u001b[0m                     \u001b[1;34m'An error occurred while calling {0}{1}{2}.\\n'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m                     format(target_id, '.', name), value)\n\u001b[0m\u001b[0;32m    301\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o2540.fit.\n: java.lang.ArrayIndexOutOfBoundsException: 1\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:183)\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:52)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:90)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:71)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:207)\n\tat java.lang.Thread.run(Thread.java:744)\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    filePath1 = \"file:///ipython/CSYE7374_ASSIGNMENT2/training_data.csv\"\n",
    "    filePath2 = \"file:///ipython/CSYE7374_ASSIGNMENT2/training_data.csv\"\n",
    "    sc = SparkContext(appName=\"MainContext\")\n",
    "    sqlContext = SQLContext(sc)\n",
    "    #rawData = importRawData(sc, filePath1,filePath2).cache()\n",
    "    trainingData, validationData = importRawData(sc, filePath1,filePath2)\n",
    "    \n",
    "    try:\n",
    "        trainingData, validationData = featureEngineering(trainingData,validationData)\n",
    "        selectClassificationModel(sc, trainingData, validationData)\n",
    "        #selectRegressionModel(sc, trainingData, validationData)\n",
    "        selectClassificationPipelineModel(sqlContext, trainingData, validationData)\n",
    "        #selectRegressionPipelineModel(sqlContext, trainingData, validationData)\n",
    "    except Exception:\n",
    "        raise\n",
    "    finally:\n",
    "        sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'/ipython/CSYE7374_ASSIGNMENT2'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
