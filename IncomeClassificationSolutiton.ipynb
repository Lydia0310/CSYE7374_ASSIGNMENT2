{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS,SVMWithSGD,LogisticRegressionWithSGD, NaiveBayes\n",
    "from pyspark.mllib.regression import LabeledPoint,LinearRegressionWithSGD,RidgeRegressionWithSGD,LassoWithSGD\n",
    "from pyspark.mllib.tree import DecisionTree, RandomForest, GradientBoostedTrees\n",
    "from pyspark.mllib.feature import StandardScaler,ChiSqSelector,Normalizer,PCA\n",
    "from pyspark.mllib.evaluation import RegressionMetrics, BinaryClassificationMetrics\n",
    "from pyspark.mllib.stat import Statistics, MultivariateStatisticalSummary\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.regression import DecisionTreeRegressor, LinearRegression, RandomForestRegressor\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, RegressionEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def importRawData(sc, trainingFile, validationFile):\n",
    "    \"\"\"\n",
    "    :param sc: Spark Context\n",
    "    :param filePath: path to data .csv file\n",
    "    :return: RDD of (LabeledPoint, index)\n",
    "    \"\"\"\n",
    "    tRdd = sc.textFile(trainingFile)\n",
    "    vRdd = sc.textFile(validationFile)\n",
    "    tCount = tRdd.count()\n",
    "    vCount = vRdd.count()\n",
    "    tRdd = tRdd.filter(lambda line:'?' not in line and line!=\"\").map(lambda line: str(line).replace(\" \",\"\").split(\",\"))\n",
    "    vRdd = vRdd.filter(lambda line:'?' not in line and line!=\"\").map(lambda line: str(line).replace(\" \",\"\").split(\",\"))\n",
    "    print \"Filtering Invaild records...\"\n",
    "    print \"Training data Input: %s records, %s remain\" % (tCount, tRdd.count())\n",
    "    print \"Validation data Input: %s records, %s remain\" % (vCount, vRdd.count())\n",
    "    transformationFunction = makeTransformationFunction(tRdd)\n",
    "    tRdd = tRdd.map(transformationFunction)\n",
    "    vRdd = vRdd.map(transformationFunction)\n",
    "    return (tRdd,vRdd)\n",
    "\n",
    "def makeTransformationFunction(rdd):\n",
    "    fCount = len(rdd.first())-1\n",
    "    indic = []\n",
    "    for i in range(fCount):\n",
    "        if rdd.first()[i].isdigit():\n",
    "            temp = rdd.map(lambda a:int(a[i]))\n",
    "            binMin = temp.min()\n",
    "            binSize = (temp.max()-binMin)/10\n",
    "            indic.append((False, [binMin, binSize]))\n",
    "        else:\n",
    "            distinct = rdd.map(lambda a:a[i]).distinct().collect()\n",
    "            indic.append((True, distinct))\n",
    "    def transformationFunction(array):\n",
    "        result = [1 if array[-1].startswith('>50K') else 0]\n",
    "        for i, f in enumerate(array):\n",
    "            if i==len(array)-1:\n",
    "                break\n",
    "            elif indic[i][0]:\n",
    "                for j in range(len(indic[i][1])):\n",
    "                    result.append(1 if f==indic[i][1][j] else 0)\n",
    "            else:\n",
    "                result.append((int(f)-indic[i][1][0])/indic[i][1][1])\n",
    "        return result\n",
    "    return transformationFunction\n",
    "\n",
    "\n",
    "def featureEngineering(trainingData,validationData, zNorm = True, l2Norm = True, categorical = True, topFeature = 20):\n",
    "    \"\"\"\n",
    "    this function is to provide feature engineering and transformation\n",
    "    including partition, normalization, feature selection, dimension reduction\n",
    "    :param rawData: RDD[Vector] of raw data\n",
    "    :param corrSelection: local matrix of correliation matrix, if provided will perform CFS\n",
    "    :param zNorm: boolean of whether to perform Z normalization or not\n",
    "    :param l2Norm: boolean of whether to perform L2 normalization or not\n",
    "    :param categorical: boolean of whether to perform chi square feature selection or not\n",
    "    :param topFeature: select top features if using chi square selection\n",
    "    :return: tuple of (RDD[LabeledPoint] trainingData, RDD[LabeledPoint] validationData)\n",
    "    \"\"\"\n",
    "    print \"==============================================\"\n",
    "    print \"FEATURE ENGINEERING\"\n",
    "    print \"==============================================\"\n",
    "    print \"partitioning...\"\n",
    "    tFeatures = trainingData.map(lambda a:a[1:])\n",
    "    vFeatures = validationData.map(lambda a:a[1:])\n",
    "    tLabel = trainingData.map(lambda a:a[0])\n",
    "    vLabel = validationData.map(lambda a:a[0])\n",
    "    print \"Feature Selection... \"\n",
    "    if categorical:\n",
    "        selector = ChiSqSelector(topFeature).fit(tLabel.zip(tFeatures).map(lambda lp:LabeledPoint(lp[0], lp[1])))\n",
    "        tFeatures = selector.transform(tFeatures)\n",
    "        vFeatures = selector.transform(vFeatures)\n",
    "    featureCount = len(tFeatures.first())\n",
    "    print \"Selected %s Features: \" % featureCount\n",
    "    \n",
    "    return (tLabel.zip(tFeatures).map(lambda lp:LabeledPoint(lp[0], lp[1])).cache(), \\\n",
    "            vLabel.zip(vFeatures).map(lambda lp:LabeledPoint(lp[0], lp[1])).cache())\n",
    "\n",
    "def selectClassificationModel(sc, trainingData, validationData):\n",
    "    \"\"\"\n",
    "    wrapper function to evaluate and select all the classification models\n",
    "    :param sc: spark context\n",
    "    :param trainingData: RDD[LabeledPoint] of training data\n",
    "    :param validationData: RDD[LabeledPoint] of validation data\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    print \"==============================================\"\n",
    "    print \"CLASSIFICATION\"\n",
    "    print \"==============================================\"\n",
    "    classificationModels = [\n",
    "                                (SVMWithSGD, {\"intercept\":True, \"regType\":None}), \n",
    "                                (SVMWithSGD, {\"intercept\":True, \"regType\":\"l1\"}), \n",
    "                                (SVMWithSGD, {\"intercept\":True, \"regType\":\"l2\"}), \n",
    "                                (LogisticRegressionWithLBFGS, {\"intercept\":True, \"regType\":\"l1\"}), \n",
    "                                (LogisticRegressionWithLBFGS, {\"intercept\":True, \"regType\":\"l2\"}), \n",
    "                                (LogisticRegressionWithSGD, {\"intercept\":True, \"regType\":None}), \n",
    "                                (LogisticRegressionWithSGD, {\"intercept\":True, \"regType\":\"l1\"}), \n",
    "                                (LogisticRegressionWithSGD, {\"intercept\":True, \"regType\":\"l2\"})\n",
    "                           ]\n",
    "    for modelClass, kwargs in classificationModels:\n",
    "        trainClassificationModel(sc, trainingData, validationData, modelClass, **kwargs)\n",
    "    #GBT is waaaaaay too slow for this dataset\n",
    "    classificationModels = [\n",
    "                                (DecisionTree, {\"numClasses\":2,\"categoricalFeaturesInfo\":{},\"minInstancesPerNode\":100 ,\"impurity\":\"gini\"}), \n",
    "                                (DecisionTree, {\"numClasses\":2,\"categoricalFeaturesInfo\":{},\"minInstancesPerNode\":100 , \"impurity\":\"entropy\"}), \n",
    "                                (RandomForest, {\"numClasses\":2,\"categoricalFeaturesInfo\":{},\"numTrees\":20, \"impurity\":\"gini\"}), \n",
    "                                (RandomForest, {\"numClasses\":2,\"categoricalFeaturesInfo\":{},\"numTrees\":20, \"impurity\":\"entropy\"}) \n",
    "                           ]\n",
    "    for modelClass, kwargs in classificationModels:\n",
    "        trainClassificationTreeModel(sc, trainingData, validationData, modelClass, **kwargs)\n",
    "\n",
    "def trainClassificationModel(sc, trainingData, validationData, modelClass, **kwargs):\n",
    "    \"\"\"\n",
    "    train classification models for NOT-TREE based model\n",
    "    :param sc: spark context\n",
    "    :param trainingData: RDD[LabeledPoint] of training data\n",
    "    :param validationData: RDD[LabeledPoint] of validation data\n",
    "    :modelClass: model CLASS that use to train\n",
    "    :kwargs: key-value paired arguments for modelClass, would be passes in directly\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    print \"Classification Model: %s %s\" % (modelClass.__name__, kwargs)\n",
    "    startTime = time.time()\n",
    "    model = modelClass.train(trainingData, **kwargs)\n",
    "    model.clearThreshold()\n",
    "    validationsResult = validationData.map(lambda lp:(float(model.predict(lp.features)), lp.label))\n",
    "    metric = BinaryClassificationMetrics(validationsResult)\n",
    "    # the error rate search is to search for overall best error rate\n",
    "    # regardless of precision and recall, however they could be evaluate by PR area and ROC area\n",
    "    errors = []\n",
    "    for i in range(1, 11):\n",
    "        err = validationsResult.filter(lambda (predict,label):(1 if predict>i/10.0 else 0)!=label).count() \\\n",
    "                                            / float(validationsResult.count())\n",
    "        errors.append((err, i/10.0))\n",
    "    errors.sort(key=lambda t:t[0])\n",
    "    print \"[ Error: %.4f\\t\\tPrecision-recall: %.4f\\tROC: %.4f ] - %s sec\" \\\n",
    "            % (errors[0][0], metric.areaUnderPR, metric.areaUnderROC, (time.time()-startTime))\n",
    "        \n",
    "def trainClassificationTreeModel(sc, trainingData, validationData, modelClass, **kwargs):\n",
    "    \"\"\"\n",
    "    train classification models for TREE based model\n",
    "    :param sc: spark context\n",
    "    :param trainingData: RDD[LabeledPoint] of training data\n",
    "    :param validationData: RDD[LabeledPoint] of validation data\n",
    "    :modelClass: model CLASS that use to train\n",
    "    :kwargs: key-value paired arguments for modelClass, would be passes in directly\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    print \"Classification Model: %s %s\" % (modelClass.__name__, kwargs)\n",
    "    startTime = time.time()\n",
    "    model = modelClass.trainClassifier(trainingData, **kwargs)\n",
    "    validationFeatures = validationData.map(lambda lp:lp.features)\n",
    "    # !!!beware, due to some stange bug, DO NOT chain RDD transformation on tree model predict, count() immediately!!!\n",
    "    validationsResult = model.predict(validationFeatures)\n",
    "    totalCount = validationsResult.count()\n",
    "    validationsResult = validationsResult.zip(validationData.map(lambda lp:lp.label))\n",
    "    errCount = validationsResult.filter(lambda (predict,label):predict!=label).count()\n",
    "    validationsResult = validationsResult.zip(validationData.map(lambda lp:lp.label))\n",
    "    err = float(errCount) / totalCount\n",
    "    print \"[ Error: %.4f ] - %s sec\" % (err, (time.time()-startTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def selectClassificationPipelineModel(sqlContext, trainingData, validationData):\n",
    "    \"\"\"\n",
    "    wrapper function to evaluate and select all the classification models using pipeline CV\n",
    "    :param sqlContext: spark sql context\n",
    "    :param trainingData: RDD[LabeledPoint] of training data\n",
    "    :param validationData: RDD[LabeledPoint] of validation data\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    print \"==============================================\"\n",
    "    print \"CLASSIFICATION WITH PIPELINE CV\"\n",
    "    print \"==============================================\"\n",
    "    classificationModels = [\n",
    "                                (LogisticRegression(), {\"fitIntercept\":True, \"regParam\":[0.1,0.01,0.001]})\n",
    "                           ]\n",
    "    for modelObject, kwargs in classificationModels:\n",
    "        trainClassificationPipelineModel(sqlContext, trainingData, validationData, modelObject, kwargs)\n",
    "    \n",
    "def trainClassificationPipelineModel(sqlContext, trainingData, validationData, modelObject, kwargs):\n",
    "    \"\"\"\n",
    "    train classification models for NOT-TREE based model\n",
    "    :param sqlContext: spark sql context\n",
    "    :param trainingData: RDD[LabeledPoint] of training data\n",
    "    :param validationData: RDD[LabeledPoint] of validation data\n",
    "    :modelObject: model OBJECT that use to train\n",
    "    :kwargs: key-value dict for cross validation, would be passes in directly\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    print \"Classification Model: %s %s\" % (modelObject.__class__.__name__, kwargs)\n",
    "    startTime = time.time()\n",
    "    trainingDF = sqlContext.createDataFrame(trainingData).cache()\n",
    "    validationDF = sqlContext.createDataFrame(validationData).cache()\n",
    "    grid = ParamGridBuilder().baseOn(kwargs).build()\n",
    "    evaluator = BinaryClassificationEvaluator()\n",
    "    cv = CrossValidator(estimator=modelObject, estimatorParamMaps=grid, evaluator=evaluator, numFolds = 5)\n",
    "    cvModel = cv.fit(trainingDF)\n",
    "    validationResult = cvModel.transform(validationDF)\n",
    "    areaUnderROC = evaluator.evaluate(validationResult, {evaluator.metricName: \"areaUnderROC\"})\n",
    "    areaUnderPR = evaluator.evaluate(validationResult, {evaluator.metricName: \"areaUnderPR\"})\n",
    "    print \"[ Precision-recall: %.4f\\tROC: %.4f ] - %s sec\" \\\n",
    "            % (areaUnderPR, areaUnderROC, (time.time()-startTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering Invaild records...\n",
      "Training data Input: 32562 records, 30162 remain\n",
      "Validation data Input: 16282 records, 15060 remain\n",
      "==============================================\n",
      "FEATURE ENGINEERING\n",
      "==============================================\n",
      "partitioning...\n",
      "Feature Selection... \n",
      "Selected 20 Features: \n",
      "==============================================\n",
      "CLASSIFICATION\n",
      "==============================================\n",
      "Classification Model: SVMWithSGD {'regType': None, 'intercept': True}\n",
      "[ Error: 0.2156\t\tPrecision-recall: 0.5730\tROC: 0.8046 ] - 5.46849608421 sec\n",
      "Classification Model: SVMWithSGD {'regType': 'l1', 'intercept': True}\n",
      "[ Error: 0.2064\t\tPrecision-recall: 0.5810\tROC: 0.8109 ] - 2.77928996086 sec\n",
      "Classification Model: SVMWithSGD {'regType': 'l2', 'intercept': True}\n",
      "[ Error: 0.2276\t\tPrecision-recall: 0.5588\tROC: 0.7971 ] - 2.861068964 sec\n",
      "Classification Model: LogisticRegressionWithLBFGS {'regType': 'l1', 'intercept': True}\n",
      "[ Error: 0.1695\t\tPrecision-recall: 0.7033\tROC: 0.8760 ] - 2.29076313972 sec\n",
      "Classification Model: LogisticRegressionWithLBFGS {'regType': 'l2', 'intercept': True}\n",
      "[ Error: 0.1663\t\tPrecision-recall: 0.7118\tROC: 0.8817 ] - 2.2789170742 sec\n",
      "Classification Model: LogisticRegressionWithSGD {'regType': None, 'intercept': True}\n",
      "[ Error: 0.2169\t\tPrecision-recall: 0.5534\tROC: 0.7985 ] - 3.32898402214 sec\n",
      "Classification Model: LogisticRegressionWithSGD {'regType': 'l1', 'intercept': True}\n",
      "[ Error: 0.2342\t\tPrecision-recall: 0.4937\tROC: 0.7689 ] - 3.20704007149 sec\n",
      "Classification Model: LogisticRegressionWithSGD {'regType': 'l2', 'intercept': True}\n",
      "[ Error: 0.2232\t\tPrecision-recall: 0.5422\tROC: 0.7929 ] - 3.15882492065 sec\n",
      "Classification Model: DecisionTree {'categoricalFeaturesInfo': {}, 'minInstancesPerNode': 100, 'numClasses': 2, 'impurity': 'gini'}\n",
      "[ Error: 0.1738 ] - 0.532620191574 sec\n",
      "Classification Model: DecisionTree {'categoricalFeaturesInfo': {}, 'minInstancesPerNode': 100, 'numClasses': 2, 'impurity': 'entropy'}\n",
      "[ Error: 0.1732 ] - 0.508826971054 sec\n",
      "Classification Model: RandomForest {'categoricalFeaturesInfo': {}, 'numTrees': 20, 'impurity': 'gini', 'numClasses': 2}\n",
      "[ Error: 0.1810 ] - 0.743314981461 sec\n",
      "Classification Model: RandomForest {'categoricalFeaturesInfo': {}, 'numTrees': 20, 'impurity': 'entropy', 'numClasses': 2}\n",
      "[ Error: 0.1783 ] - 0.743013143539 sec\n",
      "==============================================\n",
      "CLASSIFICATION WITH PIPELINE CV\n",
      "==============================================\n",
      "Classification Model: LogisticRegression {'fitIntercept': True, 'regParam': [0.1, 0.01, 0.001]}\n",
      "[ Precision-recall: 0.6970\tROC: 0.8797 ] - 5.58309006691 sec\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    filePath1 = \"file:///ipython/adult_data.csv\"\n",
    "    filePath2 = \"file:///ipython/adult.test.csv\"\n",
    "    sc = SparkContext(appName=\"MainContext\")\n",
    "    sqlContext = SQLContext(sc)\n",
    "    try:\n",
    "        trainingData, validationData = importRawData(sc, filePath1,filePath2)\n",
    "        trainingData, validationData = featureEngineering(trainingData,validationData)\n",
    "        selectClassificationModel(sc, trainingData, validationData)\n",
    "        selectClassificationPipelineModel(sqlContext, trainingData, validationData)\n",
    "    except Exception:\n",
    "        raise\n",
    "    finally:\n",
    "        sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
