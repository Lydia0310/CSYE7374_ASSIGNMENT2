{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================\n",
      "file:///ipython/spark/BBC.txt\n",
      "==============================================\n",
      "==============================================\n",
      "CLUSTERING\n",
      "==============================================\n",
      "Clustering Model: KMeans {'k': 2}\n",
      "[Commercial Rate in Dataset: 0.4749 ] - 17.0943098068 sec\n",
      "[Commercial Rate in Clustering Model: 0.7735 ] - 17.0951399803 sec\n",
      "[Correct Rate: 0.4086 ] - 17.0951769352 sec\n",
      "==============================================\n",
      "file:///ipython/spark/CNN.txt\n",
      "==============================================\n",
      "==============================================\n",
      "CLUSTERING\n",
      "==============================================\n",
      "Clustering Model: KMeans {'k': 2}\n",
      "[Commercial Rate in Dataset: 0.6392 ] - 24.2854502201 sec\n",
      "[Commercial Rate in Clustering Model: 0.5301 ] - 24.2862520218 sec\n",
      "[Correct Rate: 0.4177 ] - 24.2862770557 sec\n",
      "==============================================\n",
      "file:///ipython/spark/CNNIBN.txt\n",
      "==============================================\n",
      "==============================================\n",
      "CLUSTERING\n",
      "==============================================\n",
      "Clustering Model: KMeans {'k': 2}\n",
      "[Commercial Rate in Dataset: 0.6550 ] - 32.4633169174 sec\n",
      "[Commercial Rate in Clustering Model: 0.5144 ] - 32.4641370773 sec\n",
      "[Correct Rate: 0.4037 ] - 32.4641709328 sec\n",
      "==============================================\n",
      "file:///ipython/spark/NDTV.txt\n",
      "==============================================\n",
      "==============================================\n",
      "CLUSTERING\n",
      "==============================================\n",
      "Clustering Model: KMeans {'k': 2}\n",
      "[Commercial Rate in Dataset: 0.7368 ] - 16.0542960167 sec\n",
      "[Commercial Rate in Clustering Model: 0.6023 ] - 16.0550689697 sec\n",
      "[Correct Rate: 0.5208 ] - 16.0551059246 sec\n",
      "==============================================\n",
      "file:///ipython/spark/TIMESNOW.txt\n",
      "==============================================\n",
      "==============================================\n",
      "CLUSTERING\n",
      "==============================================\n",
      "Clustering Model: KMeans {'k': 2}\n",
      "[Commercial Rate in Dataset: 0.6407 ] - 34.1891670227 sec\n",
      "[Commercial Rate in Clustering Model: 0.5286 ] - 34.1899819374 sec\n",
      "[Correct Rate: 0.4081 ] - 34.1900179386 sec\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.mllib.stat import Statistics, MultivariateStatisticalSummary\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.mllib.clustering import KMeans, KMeansModel\n",
    "from pyspark.mllib.clustering import GaussianMixture\n",
    "import time\n",
    "\n",
    "def importRawData(sc, filePath):\n",
    "    \"\"\"\n",
    "    :param sc: Spark Context\n",
    "    :param filePath: path to data .csv file\n",
    "    :return: RDD of (LabeledPoint, index)\n",
    "    \"\"\"\n",
    "    rdd = MLUtils.loadLibSVMFile(sc,filePath)\n",
    "    # index was keep for this dataset, we need index to perform partition\n",
    "    # spark DOES NOT gurantee tranformation and action execute order for RDD[Vector]\n",
    "    return rdd\n",
    "            \n",
    "\n",
    "def exploreData(rawData, corrThreshold = 0.05, colinearThreshold = 0.8):\n",
    "    \"\"\"\n",
    "    this function is use to print out some basic statistic of data\n",
    "    :param rawData: RDD[Vector] of raw data\n",
    "    :param corrThreshold: correlation threshold for print\n",
    "    :param colinearThreshold: colinear threshold for print\n",
    "    :return: local matrix of correliation matrix, corr[0] as label, rest are features\n",
    "    \"\"\"\n",
    "    print \"==============================================\"\n",
    "    print \"EXPLORE DATA\"\n",
    "    print \"==============================================\"\n",
    "    matrix = rawData.map(lambda lp:lp.features)\n",
    "    trait = Statistics.colStats(matrix)\n",
    "    print \"Features:\"\n",
    "    print \"Format:\\t\\t[% 10s\\t% 12s\\t% 10s\\t% 12s\\t% 10s]\" % (\"mean\",\"variance\",\"numNonzeors\",\"max\",\"min\")\n",
    "    for i in range(1, len(trait.mean())):\n",
    "        print \"Feature %s:\\t[ % 10.2f\\t% 12.2f\\t% 10d\\t% 12.2f\\t% 10.2f ] \" % (i+1, trait.mean()[i], trait.variance()[i],\\\n",
    "                                                    trait.numNonzeros()[i], trait.max()[i], trait.min()[i])\n",
    "    print \"Label:\"\n",
    "    print \"Label :\\t[ % 10.2f\\t% 12.2f\\t% 10d\\t% 12.2f\\t% 10.2f ] \" % (trait.mean()[0], trait.variance()[0], \\\n",
    "                                            trait.numNonzeros()[0], trait.max()[0], trait.min()[0])\n",
    "    corr = Statistics.corr(matrix, method=\"pearson\")\n",
    "    print \"Correlation between Features and Label (only show corr > %s): \" % corrThreshold\n",
    "    labelCorr = zip([row[0] for row in corr[1:] if abs(row[0])>=corrThreshold], range(1, len(corr)))\n",
    "    labelCorr.sort(key=lambda t:abs(t[0]), reverse=True)\n",
    "    for corrScore, i in labelCorr:\n",
    "        print \"Feature %s: % 6.4f\" % (i, corrScore)\n",
    "    print \"Correlation between Features (only show colinear > %s): \" % colinearThreshold\n",
    "    for i, row in enumerate(corr):\n",
    "        for j, r in enumerate(row):\n",
    "            if (i!=0 and j!=0 and i<j and abs(r)>colinearThreshold):\n",
    "                print \"Feature %s and Feature %s have r=%.4f\" % (i, j, r)\n",
    "    return corr\n",
    "\n",
    "def featureEngineering(rawData, corrSelection = None, zNorm = True, l2Norm = True, categorical = False, topFeature = 10):\n",
    "    \"\"\"\n",
    "    this function is to provide feature engineering and transformation\n",
    "    including partition, normalization, feature selection, dimension reduction\n",
    "    :param rawData: RDD[Vector] of raw data\n",
    "    :param corrSelection: local matrix of correliation matrix, if provided will perform CFS\n",
    "    :param zNorm: boolean of whether to perform Z normalization or not\n",
    "    :param l2Norm: boolean of whether to perform L2 normalization or not\n",
    "    :param categorical: boolean of whether to perform chi square feature selection or not\n",
    "    :param topFeature: select top features if using chi square selection\n",
    "    :return: tuple of (RDD[LabeledPoint] trainingData, RDD[LabeledPoint] validationData)\n",
    "    \"\"\"\n",
    "    print \"==============================================\"\n",
    "    print \"FEATURE ENGINEERING\"\n",
    "    print \"==============================================\"\n",
    "    print \"partitioning...\"\n",
    "    # beware this partitioning is HARD CODED!\n",
    "    # because it was suggested by data set creator\n",
    "    # first 463715 used as trainning, last 51630 used as validation\n",
    "    tFeatures = rawData.filter(lambda a:a[1]<463715).map(lambda a:a[0][1:])\n",
    "    vFeatures = rawData.filter(lambda a:a[1]>=463715).map(lambda a:a[0][1:])\n",
    "    tLabel = rawData.filter(lambda a:a[1]<463715).map(lambda a:a[0][0])\n",
    "    vLabel = rawData.filter(lambda a:a[1]>=463715).map(lambda a:a[0][0])\n",
    "    print \"Normalization and Scaling... \"\n",
    "    if(zNorm):\n",
    "        zN = StandardScaler(withMean=True, withStd=True).fit(tFeatures)\n",
    "        tFeatures = zN.transform(tFeatures)\n",
    "        vFeatures = zN.transform(vFeatures)\n",
    "    if(l2Norm):\n",
    "        l2N = Normalizer()\n",
    "        tFeatures = l2N.transform(tFeatures)\n",
    "        vFeatures = l2N.transform(vFeatures)\n",
    "    print \"Feature Selection... \"\n",
    "    # only categorical value for classification problem could use chi square selector\n",
    "    # otherwise, use correlation based selector instead\n",
    "    if categorical or (corrSelection is None):\n",
    "        selector = ChiSqSelector(topFeature).fit(tFeatures)\n",
    "        tFeatures = selector.transform(tFeatures)\n",
    "        vFeatures = selector.transform(vFeatures)\n",
    "    else:\n",
    "        bestFeatures = cfs(corrSelection)\n",
    "        tFeatures = tFeatures.map(lambda a:[a[n-1] for n in bestFeatures])\n",
    "        vFeatures = vFeatures.map(lambda a:[a[n-1] for n in bestFeatures])\n",
    "    featureCount = len(tFeatures.first())\n",
    "    print \"Selected %s Features: \" % featureCount\n",
    "    print \"Dimension Reduction...\"\n",
    "    # PCA dimension reduction = EVD of cov matrix = SVD of data matrix\n",
    "    # spark use EVD implement, hence we need to calculate eigen vector ourself to decide cutoff\n",
    "    # cutoff point the reduced dimension could represent 90% of original total variance\n",
    "    selector = PCA(len(bestFeatures)).fit(tFeatures.map(lambda a: LabeledPoint(1,a).features))\n",
    "    temp = selector.transform(tFeatures)\n",
    "    eigen = eigenvalues(temp)\n",
    "    print \"PCA Eigen Vector: %s\" % eigen\n",
    "    accVarPor = [sum(eigen[:i+1])/sum(eigen) for i in range(len(eigen))]\n",
    "    print \"Accumulate Variance Porpotion:\"\n",
    "    reduction = 0\n",
    "    for i in range(len(accVarPor)):\n",
    "        print \"%s Dimensions:  %.2f%%\" % (i+1, accVarPor[i]*100)\n",
    "        if(reduction==0 and accVarPor[i]>0.9): reduction = i+1\n",
    "    print \"Reduce to %s dimensions...\" % reduction\n",
    "    selector = PCA(reduction).fit(tFeatures.map(lambda a: LabeledPoint(1,a).features))\n",
    "    return (tLabel.zip(tFeatures).map(lambda lp:LabeledPoint(lp[0], lp[1])).cache(), \\\n",
    "            vLabel.zip(vFeatures).map(lambda lp:LabeledPoint(lp[0], lp[1])).cache())\n",
    "\n",
    "def cfs(corr):\n",
    "    \"\"\"\n",
    "    helper function of correlation feature selection, implemented in greedy algorithm O(n^2logn)\n",
    "    :param corr: local matrix of correliation matrix\n",
    "    :return: local list of best k features, list element as feature index (1 based)\n",
    "    \"\"\"\n",
    "    features = range(1,len(corr))\n",
    "    bestK = []\n",
    "    queue = []\n",
    "    merit = -1\n",
    "    sumL = 0\n",
    "    sumF = 0\n",
    "    print \"Greedy Correlation Feature Selection (CFS)\"\n",
    "    # the stop condition of this algorithm is we reach maximum merit\n",
    "    # i.e. merit start to decrease as we include more features\n",
    "    # because of the increase correlation between features-to-features outweight featur-to-labels'\n",
    "    # that's a bad thing to avoid, hence we stop there\n",
    "    while(len(features)>0):\n",
    "        for f in features:\n",
    "            bestK.append(f)\n",
    "            tempL = sumL+abs(corr[0][f])\n",
    "            tempF = sumF\n",
    "            for cur in bestK:\n",
    "                if cur!=f:\n",
    "                    tempF += abs(corr[cur][f])\n",
    "            queue.append((tempL/((len(bestK)+1+2*tempF)**0.5), f, tempL, tempF))\n",
    "            bestK.remove(f)\n",
    "        queue.sort(key=lambda e:e[0], reverse=True)\n",
    "        if(queue[0][0]<=merit):\n",
    "            break\n",
    "        merit = queue[0][0]\n",
    "        bestK.append(queue[0][1])\n",
    "        features.remove(queue[0][1])\n",
    "        sumL = queue[0][2]\n",
    "        sumF = queue[0][3]\n",
    "        print \"k=%s\\tHighest Merit=%.4f\\tSelected:%s\" % (len(bestK),merit,bestK)\n",
    "    return bestK\n",
    "\n",
    "def eigenvalues(data):\n",
    "    \"\"\"\n",
    "    helper function of calculate eigen vector of PCA, to determine how many dimension after reduction\n",
    "    :param data: RDD[Vector] of data after PCA transformation\n",
    "    :return: local list of eigen vector based on input data\n",
    "    \"\"\"\n",
    "    # only eigen value is calculated, not the whole covariance because that would be too slow\n",
    "    count = data.count()\n",
    "    miu = data.reduce(lambda a,b:[a[i]+b[i] for i in range(len(a))])\n",
    "    miu = map(lambda s:s/count, miu)\n",
    "    eigen = data.map(lambda a:[(a[i]-miu[i])**2 for i in range(len(a))]) \\\n",
    "                .reduce(lambda a,b:[a[i]+b[i] for i in range(len(a))])\n",
    "    eigen = map(lambda s:s/count, eigen)\n",
    "    return eigen\n",
    "\n",
    "def selectClusteringModel(sc, trainingData):\n",
    "    \"\"\"\n",
    "    wrapper function to evaluate and select all the classification models\n",
    "    :param sc: spark context\n",
    "    :param trainingData: RDD[LabeledPoint] of training data\n",
    "    :param validationData: RDD[LabeledPoint] of validation data\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    print \"==============================================\"\n",
    "    print \"CLUSTERING\"\n",
    "    print \"==============================================\"\n",
    "    clusteringModels = [\n",
    "                                (KMeans, {\"k\":2})\n",
    "                                #,(GaussianMixture,{\"k\":2})\n",
    "                           ]\n",
    "    for modelClass, kwargs in clusteringModels:\n",
    "        trainClusteringModel(sc, trainingData, modelClass, **kwargs)\n",
    "\n",
    "def trainClusteringModel(sc, trainingData, modelClass, **kwargs):\n",
    "    \"\"\"\n",
    "    train classification models for NOT-TREE based model\n",
    "    :param sc: spark context\n",
    "    :param trainingData: RDD[LabeledPoint] of training data\n",
    "    :param validationData: RDD[LabeledPoint] of validation data\n",
    "    :modelClass: model CLASS that use to train\n",
    "    :kwargs: key-value paired arguments for modelClass, would be passes in directly\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    print \"Clustering Model: %s %s\" % (modelClass.__name__, kwargs)\n",
    "    startTime = time.time()\n",
    "    data = trainingData.map(lambda lp:lp.features)\n",
    "   \n",
    "    model = modelClass.train(data, **kwargs)\n",
    "    validationsResult = trainingData.map(lambda lp:(float(model.predict(lp.features)), lp.label))\n",
    "    #errorRate = validationsResult.filter(lambda t:t[0]==t[1]).count()/float(validationsResult.count())\n",
    "    rate2 = validationsResult.filter(lambda t:t[0]==0).count()/float(validationsResult.count())\n",
    "    rate1 = validationsResult.filter(lambda t:t[1]==1).count()/float(validationsResult.count())\n",
    "    rate3 = validationsResult.filter(lambda t:(t[1]==1)&(t[0]==0)).count()/float(validationsResult.count())\n",
    "    print \"[Commercial Rate in Dataset: %.4f ] - %s sec\" \\\n",
    "            % (rate1, (time.time()-startTime))\n",
    "    print \"[Commercial Rate in Clustering Model: %.4f ] - %s sec\" \\\n",
    "            % (rate2, (time.time()-startTime))\n",
    "    print \"[Correct Rate: %.4f ] - %s sec\" \\\n",
    "            % (rate3, (time.time()-startTime))\n",
    "    #print \"[Rate: %.4f ] - %s sec\" \\\n",
    "     #       % (errorRate, (time.time()-startTime))\n",
    "    #metric = model.computeCost(data)\n",
    "  \n",
    "    #print \"[ WSSSE: %.4f ] - %s sec\" \\\n",
    "     #       % (metric, (time.time()-startTime))\n",
    "        \n",
    "        \n",
    "if __name__==\"__main__\":\n",
    "    \n",
    "    fileName = [\"file:///ipython/spark/BBC.txt\",\"file:///ipython/spark/CNN.txt\",\"file:///ipython/spark/CNNIBN.txt\",\"file:///ipython/spark/NDTV.txt\",\"file:///ipython/spark/TIMESNOW.txt\"]\n",
    "    for filePath in fileName:\n",
    "        print \"==============================================\"\n",
    "        print filePath \n",
    "        print \"==============================================\"\n",
    "        sc = SparkContext(appName=\"MainCOntext\")\n",
    "        rawData = importRawData(sc, filePath).cache()\n",
    "        try:\n",
    "            #corrSelection = exploreData(rawData)\n",
    "            selectClusteringModel(sc, rawData)\n",
    "        except Exception:\n",
    "            raise\n",
    "        finally:\n",
    "            sc.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
