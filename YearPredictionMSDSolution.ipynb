{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================\n",
      "EXPLORE DATA\n",
      "==============================================\n",
      "Features:\n",
      "Format:\t\t[      mean\t    variance\tnumNonzeors\t         max\t       min]\n",
      "Feature 2:\t[      43.39\t       36.82\t    515345\t       61.97\t      1.75 ] \n",
      "Feature 3:\t[       1.29\t     2660.53\t    515345\t      384.07\t   -337.09 ] \n",
      "Feature 4:\t[       8.66\t     1243.87\t    515345\t      322.85\t   -301.01 ] \n",
      "Feature 5:\t[       1.16\t      266.43\t    515345\t      335.77\t   -154.18 ] \n",
      "Feature 6:\t[      -6.55\t      522.62\t    515345\t      262.07\t   -181.95 ] \n",
      "Feature 7:\t[      -9.52\t      165.32\t    515345\t      166.24\t    -81.79 ] \n",
      "Feature 8:\t[      -2.39\t      212.34\t    515345\t      172.40\t   -188.21 ] \n",
      "Feature 9:\t[      -1.79\t       63.42\t    515345\t      126.74\t    -72.50 ] \n",
      "Feature 10:\t[       3.73\t      112.00\t    515345\t      146.30\t   -126.48 ] \n",
      "Feature 11:\t[       1.88\t       42.64\t    515345\t       60.35\t    -41.63 ] \n",
      "Feature 12:\t[      -0.15\t       19.10\t    515343\t       88.02\t    -69.68 ] \n",
      "Feature 13:\t[       2.55\t       69.23\t    515345\t       87.91\t    -94.04 ] \n",
      "Feature 14:\t[      33.71\t      495.49\t    515345\t      549.76\t      0.13 ] \n",
      "Feature 15:\t[    2439.36\t  3060287.31\t    515345\t    65735.78\t      8.47 ] \n",
      "Feature 16:\t[    1967.73\t  1591343.88\t    515345\t    36816.79\t     21.21 ] \n",
      "Feature 17:\t[    1514.86\t  1194279.69\t    515345\t    31849.49\t     17.86 ] \n",
      "Feature 18:\t[     910.98\t   226298.29\t    515345\t    19865.93\t     12.15 ] \n",
      "Feature 19:\t[     879.15\t   332774.37\t    515345\t    16831.95\t      5.52 ] \n",
      "Feature 20:\t[     603.74\t   100805.80\t    515345\t    11901.71\t     19.81 ] \n",
      "Feature 21:\t[     517.58\t    95706.54\t    515345\t     9569.78\t      6.25 ] \n",
      "Feature 22:\t[     393.96\t    45801.78\t    515345\t     9616.62\t      6.18 ] \n",
      "Feature 23:\t[     325.73\t    27456.29\t    515345\t     3721.87\t     15.31 ] \n",
      "Feature 24:\t[     288.89\t    34954.31\t    515345\t     6737.12\t      6.12 ] \n",
      "Feature 25:\t[     291.97\t    23554.77\t    515345\t     9813.23\t      5.18 ] \n",
      "Feature 26:\t[      43.03\t    14745.12\t    515345\t     2049.60\t  -2821.43 ] \n",
      "Feature 27:\t[      43.31\t   512311.16\t    515345\t    24479.66\t -13390.36 ] \n",
      "Feature 28:\t[     -46.45\t   296894.07\t    515345\t    14505.34\t -12017.09 ] \n",
      "Feature 29:\t[     -27.67\t    47684.79\t    515345\t     3410.62\t  -4324.86 ] \n",
      "Feature 30:\t[      14.96\t    26639.39\t    515345\t     3277.63\t  -3357.28 ] \n",
      "Feature 31:\t[      44.51\t    18194.73\t    515345\t     3553.18\t  -3115.37 ] \n",
      "Feature 32:\t[       5.13\t     9818.46\t    515345\t     2347.41\t  -3805.67 ] \n",
      "Feature 33:\t[      24.03\t     5156.67\t    515345\t     1954.36\t  -1516.36 ] \n",
      "Feature 34:\t[       9.50\t     5537.10\t    515345\t     2887.85\t  -1679.12 ] \n",
      "Feature 35:\t[      -4.18\t     2864.07\t    515345\t     2330.33\t  -1590.64 ] \n",
      "Feature 36:\t[       0.50\t     1805.43\t    515345\t     1813.24\t   -989.65 ] \n",
      "Feature 37:\t[      72.65\t    11645.14\t    515345\t     2496.12\t  -1711.48 ] \n",
      "Feature 38:\t[     -51.44\t   172257.45\t    515345\t    14149.00\t  -8448.19 ] \n",
      "Feature 39:\t[     117.92\t   205570.86\t    515345\t     8059.15\t -10095.73 ] \n",
      "Feature 40:\t[    -189.88\t    67936.01\t    515345\t     6065.05\t  -9803.76 ] \n",
      "Feature 41:\t[      23.10\t    42328.78\t    515345\t     8360.15\t  -7882.82 ] \n",
      "Feature 42:\t[      -1.28\t    14359.92\t    515345\t     3537.50\t  -4673.36 ] \n",
      "Feature 43:\t[      18.15\t    14363.00\t    515345\t     3892.12\t  -4175.41 ] \n",
      "Feature 44:\t[     -51.96\t     5420.26\t    515345\t     1202.49\t  -4975.38 ] \n",
      "Feature 45:\t[       3.23\t     1472.55\t    515345\t     1830.54\t  -1072.96 ] \n",
      "Feature 46:\t[      -1.49\t     1726.01\t    515345\t      746.71\t  -1021.29 ] \n",
      "Feature 47:\t[       6.33\t     3022.18\t    515345\t     1198.63\t  -1329.96 ] \n",
      "Feature 48:\t[      78.70\t   221461.91\t    515345\t     9059.76\t -14861.70 ] \n",
      "Feature 49:\t[     142.70\t    68838.05\t    515345\t     6967.64\t  -3992.69 ] \n",
      "Feature 50:\t[     -86.52\t    43904.14\t    515345\t     6172.35\t  -6642.40 ] \n",
      "Feature 51:\t[      25.24\t    14921.51\t    515345\t     2067.20\t  -2344.53 ] \n",
      "Feature 52:\t[       6.38\t     8752.66\t    515345\t     1426.85\t  -2270.81 ] \n",
      "Feature 53:\t[      28.29\t     5632.39\t    515345\t     2460.43\t  -1746.48 ] \n",
      "Feature 54:\t[      12.77\t     4897.46\t    515345\t     2394.66\t  -3188.18 ] \n",
      "Feature 55:\t[       1.70\t     6935.34\t    515345\t     2900.52\t  -2199.78 ] \n",
      "Feature 56:\t[     -10.21\t     3341.60\t    515345\t      569.14\t  -1694.26 ] \n",
      "Feature 57:\t[      64.10\t    74981.33\t    515345\t     6955.41\t  -5154.02 ] \n",
      "Feature 58:\t[     104.82\t    96805.17\t    515345\t    12700.01\t  -5111.60 ] \n",
      "Feature 59:\t[      -0.03\t    71168.38\t    515345\t    13001.26\t  -4730.60 ] \n",
      "Feature 60:\t[      38.68\t    28585.40\t    515345\t     5419.28\t  -3756.49 ] \n",
      "Feature 61:\t[     -27.99\t    20759.92\t    515345\t     5690.29\t  -2499.95 ] \n",
      "Feature 62:\t[       3.30\t     3533.97\t    515345\t     1811.23\t  -1900.10 ] \n",
      "Feature 63:\t[       0.31\t     2418.81\t    515345\t      973.05\t  -1396.70 ] \n",
      "Feature 64:\t[      -0.48\t     1419.44\t    515344\t      812.42\t   -600.09 ] \n",
      "Feature 65:\t[    -138.22\t    94979.47\t    515345\t    11048.20\t -10345.83 ] \n",
      "Feature 66:\t[      -0.70\t    49372.18\t    515345\t     2877.74\t  -7375.98 ] \n",
      "Feature 67:\t[       0.24\t    16420.11\t    515345\t     3447.48\t  -3896.28 ] \n",
      "Feature 68:\t[       3.15\t     9984.20\t    515345\t     2055.04\t  -1199.00 ] \n",
      "Feature 69:\t[      27.64\t    13618.00\t    515345\t     4779.80\t  -2564.79 ] \n",
      "Feature 70:\t[      31.82\t    11312.44\t    515345\t     5286.82\t  -1904.98 ] \n",
      "Feature 71:\t[      -0.84\t     1354.18\t    515345\t      745.50\t   -974.70 ] \n",
      "Feature 72:\t[      -8.93\t    63305.96\t    515345\t     3958.07\t  -7057.71 ] \n",
      "Feature 73:\t[       4.85\t    52468.68\t    515345\t     4741.18\t  -6953.36 ] \n",
      "Feature 74:\t[     -27.35\t    26824.75\t    515345\t     2124.10\t  -8400.60 ] \n",
      "Feature 75:\t[     -11.94\t     4005.48\t    515344\t     1639.93\t  -1812.89 ] \n",
      "Feature 76:\t[     -21.57\t     4185.00\t    515345\t     1278.33\t  -1387.51 ] \n",
      "Feature 77:\t[      -5.58\t      694.75\t    515345\t      741.03\t   -718.42 ] \n",
      "Feature 78:\t[     -23.30\t    71926.37\t    515345\t    10020.28\t  -9831.45 ] \n",
      "Feature 79:\t[      31.11\t    20781.01\t    515345\t     3423.60\t  -2025.78 ] \n",
      "Feature 80:\t[    -104.97\t    40456.30\t    515345\t     5188.33\t  -8390.04 ] \n",
      "Feature 81:\t[      26.96\t    15338.30\t    515345\t     3735.03\t  -4754.94 ] \n",
      "Feature 82:\t[      15.76\t     1030.39\t    515345\t      840.97\t   -437.72 ] \n",
      "Feature 83:\t[     -73.46\t    30841.99\t    515345\t     4469.45\t  -4402.38 ] \n",
      "Feature 84:\t[      41.54\t    14939.88\t    515345\t     3210.70\t  -1810.69 ] \n",
      "Feature 85:\t[      37.93\t     9034.62\t    515345\t     1734.08\t  -3098.35 ] \n",
      "Feature 86:\t[       0.32\t      261.20\t    515345\t      260.54\t   -341.79 ] \n",
      "Feature 87:\t[      17.67\t    13093.75\t    515345\t     3662.07\t  -3168.92 ] \n",
      "Feature 88:\t[     -26.32\t    30268.11\t    515345\t     2833.61\t  -4319.99 ] \n",
      "Feature 89:\t[       4.46\t      178.13\t    515345\t      463.42\t   -236.04 ] \n",
      "Feature 90:\t[      20.04\t    34431.86\t    515345\t     7393.40\t  -7458.38 ] \n",
      "Feature 91:\t[       1.33\t      487.91\t    515345\t      677.90\t   -381.42 ] \n",
      "Label:\n",
      "Label :\t[    1998.40\t      119.49\t    515345\t     2011.00\t   1922.00 ] \n",
      "Correlation between Features and Label: \n",
      "Feature 1:  1.0000\n",
      "Feature 2:  0.2254\n",
      "Feature 7: -0.1874\n",
      "Feature 4: -0.1395\n",
      "Feature 64: -0.1263\n",
      "Feature 41: -0.1240\n",
      "Feature 8:  0.1105\n",
      "Feature 68: -0.1028\n",
      "Feature 47: -0.1013\n",
      "Feature 37: -0.1010\n",
      "Feature 70: -0.0995\n",
      "Feature 13: -0.0972\n",
      "Feature 58: -0.0970\n",
      "Feature 48:  0.0961\n",
      "Feature 60: -0.0927\n",
      "Feature 34: -0.0912\n",
      "Feature 79: -0.0874\n",
      "Feature 21:  0.0871\n",
      "Feature 74:  0.0838\n",
      "Feature 75:  0.0817\n",
      "Feature 69:  0.0813\n",
      "Feature 15:  0.0787\n",
      "Feature 39:  0.0744\n",
      "Feature 54:  0.0741\n",
      "Feature 32: -0.0739\n",
      "Feature 30:  0.0733\n",
      "Feature 40:  0.0703\n",
      "Feature 73: -0.0683\n",
      "Feature 61: -0.0672\n",
      "Feature 26:  0.0647\n",
      "Feature 51: -0.0629\n",
      "Feature 42: -0.0608\n",
      "Feature 86:  0.0595\n",
      "Feature 25: -0.0582\n",
      "Feature 22: -0.0567\n",
      "Feature 67: -0.0544\n",
      "Feature 87: -0.0535\n",
      "Feature 53: -0.0531\n",
      "Feature 23:  0.0497\n",
      "Feature 55: -0.0476\n",
      "Feature 71:  0.0470\n",
      "Feature 49: -0.0468\n",
      "Feature 24:  0.0464\n",
      "Feature 36: -0.0440\n",
      "Feature 63: -0.0436\n",
      "Feature 35:  0.0430\n",
      "Feature 38: -0.0413\n",
      "Feature 82: -0.0401\n",
      "Feature 66: -0.0390\n",
      "Feature 89: -0.0374\n",
      "Feature 9: -0.0350\n",
      "Feature 11:  0.0332\n",
      "Feature 17:  0.0328\n",
      "Feature 50: -0.0324\n",
      "Feature 80:  0.0314\n",
      "Feature 18:  0.0307\n",
      "Feature 12:  0.0305\n",
      "Feature 76:  0.0285\n",
      "Feature 85:  0.0276\n",
      "Feature 33:  0.0270\n",
      "Feature 52:  0.0268\n",
      "Feature 62: -0.0249\n",
      "Feature 10: -0.0248\n",
      "Feature 88:  0.0246\n",
      "Feature 14:  0.0233\n",
      "Feature 28: -0.0232\n",
      "Feature 19:  0.0220\n",
      "Feature 3:  0.0214\n",
      "Feature 6:  0.0193\n",
      "Feature 27: -0.0181\n",
      "Feature 31:  0.0179\n",
      "Feature 43:  0.0143\n",
      "Feature 78: -0.0141\n",
      "Feature 84:  0.0140\n",
      "Feature 45:  0.0130\n",
      "Feature 77:  0.0128\n",
      "Feature 46:  0.0127\n",
      "Feature 56:  0.0111\n",
      "Feature 29:  0.0088\n",
      "Feature 90:  0.0084\n",
      "Feature 16: -0.0072\n",
      "Feature 57:  0.0072\n",
      "Feature 72: -0.0060\n",
      "Feature 59: -0.0050\n",
      "Feature 81: -0.0049\n",
      "Feature 20: -0.0045\n",
      "Feature 5: -0.0033\n",
      "Feature 83:  0.0025\n",
      "Feature 65:  0.0004\n",
      "Feature 44: -0.0004\n",
      "Correlation between Features (only show colinear > 0.8): \n",
      "Feature 16 and Feature 18 have r=0.8096\n",
      "Feature 16 and Feature 23 have r=0.8466\n",
      "Feature 18 and Feature 23 have r=0.8596\n",
      "Feature 20 and Feature 22 have r=0.8657\n",
      "Suggest Feature Selection based on Correlation >= 0.1: \n",
      "Features: [1, 2, 7, 4, 64, 41, 8, 68, 47, 37]\n",
      "==============================================\n",
      "FEATURE ENGINEERING\n",
      "==============================================\n",
      "partitioning...\n",
      "Normalization and Scaling... \n",
      "Feature Selection... \n",
      "Selected 10 Features\n",
      "Dimension Reduction...\n",
      "PCA Eigen Vector: [0.028584951081582193, 0.019367093494042874, 0.01541689462253865, 0.012050111547756611, 0.0099507949238018534, 0.0092517588630231917, 0.0081280802033589245, 0.0069285459651011114, 0.0061843780029987973, 0.0056798657670234804]\n",
      "Accumulate Variance Porpotion:\n",
      "1 Features:  23.52%\n",
      "2 Features:  39.45%\n",
      "3 Features:  52.14%\n",
      "4 Features:  62.05%\n",
      "5 Features:  70.24%\n",
      "6 Features:  77.85%\n",
      "7 Features:  84.54%\n",
      "8 Features:  90.24%\n",
      "9 Features:  95.33%\n",
      "10 Features:  100.00%\n",
      "Select 8 Features...\n",
      "==============================================\n",
      "CLASSIFICATION\n",
      "==============================================\n",
      "Classification Model: SVMWithSGD {'regType': None, 'intercept': True}\n",
      "[ Error: 0.0116\t\tPrecision-recall: 0.9968\tROC: 0.7954 ] - 34.8714661598 sec\n",
      "Classification Model: SVMWithSGD {'regType': 'l1', 'intercept': True}\n",
      "[ Error: 0.0116\t\tPrecision-recall: 0.9942\tROC: 0.5000 ] - 16.6166710854 sec\n",
      "Classification Model: SVMWithSGD {'regType': 'l2', 'intercept': True}\n",
      "[ Error: 0.0116\t\tPrecision-recall: 0.9968\tROC: 0.7957 ] - 15.4966700077 sec\n",
      "Classification Model: LogisticRegressionWithLBFGS {'regType': 'l1', 'intercept': True}\n",
      "[ Error: 0.0116\t\tPrecision-recall: 0.9962\tROC: 0.7626 ] - 16.6790101528 sec\n",
      "Classification Model: LogisticRegressionWithLBFGS {'regType': 'l2', 'intercept': True}\n",
      "[ Error: 0.0116\t\tPrecision-recall: 0.9968\tROC: 0.8033 ] - 14.1729390621 sec\n",
      "Classification Model: LogisticRegressionWithSGD {'regType': None, 'intercept': True}\n",
      "[ Error: 0.0116\t\tPrecision-recall: 0.9965\tROC: 0.7815 ] - 19.2994570732 sec\n",
      "Classification Model: LogisticRegressionWithSGD {'regType': 'l1', 'intercept': True}\n",
      "[ Error: 0.0116\t\tPrecision-recall: 0.9942\tROC: 0.5000 ] - 19.6149408817 sec\n",
      "Classification Model: LogisticRegressionWithSGD {'regType': 'l2', 'intercept': True}\n",
      "[ Error: 0.0116\t\tPrecision-recall: 0.9965\tROC: 0.7805 ] - 19.1292159557 sec\n",
      "Classification Model: DecisionTree {'categoricalFeaturesInfo': {}, 'minInstancesPerNode': 100, 'numClasses': 2, 'impurity': 'gini'}\n",
      "[ Error: 0.0116 ] - 3.29938006401 sec\n",
      "Classification Model: DecisionTree {'categoricalFeaturesInfo': {}, 'minInstancesPerNode': 100, 'numClasses': 2, 'impurity': 'entropy'}\n",
      "[ Error: 0.0116 ] - 3.52141785622 sec\n",
      "Classification Model: RandomForest {'categoricalFeaturesInfo': {}, 'numTrees': 20, 'impurity': 'gini', 'numClasses': 2}\n",
      "[ Error: 0.0116 ] - 5.29372692108 sec\n",
      "Classification Model: RandomForest {'categoricalFeaturesInfo': {}, 'numTrees': 20, 'impurity': 'entropy', 'numClasses': 2}\n",
      "[ Error: 0.0116 ] - 5.54645109177 sec\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS,SVMWithSGD,LogisticRegressionWithSGD, NaiveBayes\n",
    "from pyspark.mllib.regression import LabeledPoint,LinearRegressionWithSGD,RidgeRegressionWithSGD,LassoWithSGD\n",
    "from pyspark.mllib.tree import DecisionTree, RandomForest, GradientBoostedTrees\n",
    "from pyspark.mllib.feature import StandardScaler,ChiSqSelector,Normalizer,PCA\n",
    "from pyspark.mllib.evaluation import RegressionMetrics, BinaryClassificationMetrics\n",
    "from pyspark.mllib.stat import Statistics, MultivariateStatisticalSummary\n",
    "from pyspark.mllib.util import MLUtils\n",
    "import time\n",
    "\n",
    "def importRawData(sc, filePath):\n",
    "    '''\n",
    "    :param sc: Spark Context\n",
    "    :param filePath: path to data .csv file\n",
    "    :return: RDD of (LabeledPoint, index)\n",
    "    '''\n",
    "    rdd = sc.textFile(filePath)\n",
    "    return rdd.map(lambda line: line.split(\",\")) \\\n",
    "                .map(lambda array: [float(n) for n in array]) \\\n",
    "                .zipWithIndex()\n",
    "\n",
    "def exploreData(rawData, corrThreshold = 0.1, colinearThreshold = 0.8):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    print \"==============================================\"\n",
    "    print \"EXPLORE DATA\"\n",
    "    print \"==============================================\"\n",
    "    matrix = rawData.map(lambda t:t[0])\n",
    "    trait = Statistics.colStats(matrix)\n",
    "    print \"Features:\"\n",
    "    print \"Format:\\t\\t[% 10s\\t% 12s\\t% 10s\\t% 12s\\t% 10s]\" % (\"mean\",\"variance\",\"numNonzeors\",\"max\",\"min\")\n",
    "    for i in range(1, len(trait.mean())):\n",
    "        print \"Feature %s:\\t[ % 10.2f\\t% 12.2f\\t% 10d\\t% 12.2f\\t% 10.2f ] \" % (i+1, trait.mean()[i], trait.variance()[i],\\\n",
    "                                                    trait.numNonzeros()[i], trait.max()[i], trait.min()[i])\n",
    "    print \"Label:\"\n",
    "    print \"Label :\\t[ % 10.2f\\t% 12.2f\\t% 10d\\t% 12.2f\\t% 10.2f ] \" % (trait.mean()[0], trait.variance()[0], \\\n",
    "                                            trait.numNonzeros()[0], trait.max()[0], trait.min()[0])\n",
    "    corr = Statistics.corr(matrix, method=\"pearson\")\n",
    "    print \"Correlation between Features and Label: \"\n",
    "    labelCorr = zip([row[0] for row in corr], range(1, len(corr)))\n",
    "    labelCorr.sort(key=lambda t:abs(t[0]), reverse=True)\n",
    "    selection = [];\n",
    "    for corrScore, i in labelCorr:\n",
    "        print \"Feature %s: % 6.4f\" % (i, corrScore)\n",
    "        if(abs(corrScore)>=corrThreshold):\n",
    "            selection.append(i)\n",
    "    print \"Correlation between Features (only show colinear > %s): \" % colinearThreshold\n",
    "    for i, row in enumerate(corr):\n",
    "        for j, r in enumerate(row):\n",
    "            if (i!=0 and j!=0 and i<j and abs(r)>colinearThreshold):\n",
    "                print \"Feature %s and Feature %s have r=%.4f\" % (i, j, r)\n",
    "                if(i in selection and j in selection):\n",
    "                    selection.remove(j)\n",
    "    print \"Suggest Feature Selection based on Correlation >= %s: \" % corrThreshold\n",
    "    print \"Features: %s\" % selection\n",
    "    return selection\n",
    "    \n",
    "                \n",
    "def featureEngineering(rawData, selection = None, zNorm = True, l2Norm = True, categorical = False, topFeature = 10):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    #this partition is set by the suggestion of data set\n",
    "    #first 463715 used as trainning, last 51630 used as validation\n",
    "    print \"==============================================\"\n",
    "    print \"FEATURE ENGINEERING\"\n",
    "    print \"==============================================\"\n",
    "    print \"partitioning...\"\n",
    "    tFeatures = rawData.filter(lambda a:a[1]<463715).map(lambda a:a[0][1:])\n",
    "    vFeatures = rawData.filter(lambda a:a[1]>=463715).map(lambda a:a[0][1:])\n",
    "    tLabel = rawData.filter(lambda a:a[1]<463715).map(lambda a:a[0][0])\n",
    "    vLabel = rawData.filter(lambda a:a[1]>=463715).map(lambda a:a[0][0])\n",
    "    print \"Normalization and Scaling... \"\n",
    "    if(zNorm):\n",
    "        zN = StandardScaler(withMean=True, withStd=True).fit(tFeatures)\n",
    "        tFeatures = zN.transform(tFeatures)\n",
    "        vFeatures = zN.transform(vFeatures)\n",
    "    if(l2Norm):\n",
    "        l2N = Normalizer()\n",
    "        tFeatures = l2N.transform(tFeatures)\n",
    "        vFeatures = l2N.transform(vFeatures)\n",
    "    print \"Feature Selection... \"\n",
    "    if categorical and (selection == None):\n",
    "        selector = ChiSqSelector(topFeature).fit(tFeatures)\n",
    "        tFeatures = selector.transform(tFeatures)\n",
    "        vFeatures = selector.transform(vFeatures)\n",
    "    else:\n",
    "        tFeatures = tFeatures.map(lambda a:[a[n-1] for n in selection])\n",
    "        vFeatures = vFeatures.map(lambda a:[a[n-1] for n in selection])\n",
    "    featureCount = len(tFeatures.first())\n",
    "    print \"Selected %s Features\" % featureCount\n",
    "    print \"Dimension Reduction...\"\n",
    "    selector = PCA(featureCount).fit(tFeatures.map(lambda a: LabeledPoint(1,a).features))\n",
    "    temp = selector.transform(tFeatures)\n",
    "    eigen = eigenvalues(temp)\n",
    "    print \"PCA Eigen Vector: %s\" % eigen\n",
    "    accVarPor = [sum(eigen[:i+1])/sum(eigen) for i in range(len(eigen))]\n",
    "    print \"Accumulate Variance Porpotion:\"\n",
    "    reduction = 0\n",
    "    for i in range(len(accVarPor)):\n",
    "        print \"%s Features:  %.2f%%\" % (i+1, accVarPor[i]*100)\n",
    "        if(reduction==0 and accVarPor[i]>0.9): reduction = i+1\n",
    "    print \"Select %s Features...\" % reduction\n",
    "    selector = PCA(reduction).fit(tFeatures.map(lambda a: LabeledPoint(1,a).features))\n",
    "    return (tLabel.zip(tFeatures).map(lambda lp:LabeledPoint(lp[0], lp[1])).cache(), \\\n",
    "            vLabel.zip(vFeatures).map(lambda lp:LabeledPoint(lp[0], lp[1])).cache())\n",
    "    \n",
    "# def cov(data):\n",
    "#     count = data.count()\n",
    "#     miu = data.reduce(lambda a,b:[a[i]+b[i] for i in range(len(a))])\n",
    "#     miu = map(lambda s:s/count, miu)\n",
    "#     cov_ = []\n",
    "#     for i in range(len(miu)):\n",
    "#         temp = data.map(lambda a:[(a[i]-miu[i])*(a[j]-miu[j]) for j in range(len(a))])\\\n",
    "#                     .reduce(lambda a,b:[a[i]+b[i] for i in range(len(a))])\n",
    "#         temp = map(lambda s:s/count, temp)\n",
    "#         cov_.append(temp)\n",
    "#     return cov_\n",
    "\n",
    "def eigenvalues(data):\n",
    "    count = data.count()\n",
    "    miu = data.reduce(lambda a,b:[a[i]+b[i] for i in range(len(a))])\n",
    "    miu = map(lambda s:s/count, miu)\n",
    "    eigen = data.map(lambda a:[(a[i]-miu[i])**2 for i in range(len(a))]) \\\n",
    "                .reduce(lambda a,b:[a[i]+b[i] for i in range(len(a))])\n",
    "    eigen = map(lambda s:s/count, eigen)\n",
    "    return eigen\n",
    "\n",
    "def selectClassificationModel(sc, trainingData, validationData):\n",
    "    print \"==============================================\"\n",
    "    print \"CLASSIFICATION\"\n",
    "    print \"==============================================\"\n",
    "    classificationModels = [\n",
    "                                (SVMWithSGD, {\"intercept\":True, \"regType\":None}), \n",
    "                                (SVMWithSGD, {\"intercept\":True, \"regType\":\"l1\"}), \n",
    "                                (SVMWithSGD, {\"intercept\":True, \"regType\":\"l2\"}), \n",
    "                                (LogisticRegressionWithLBFGS, {\"intercept\":True, \"regType\":\"l1\"}), \n",
    "                                (LogisticRegressionWithLBFGS, {\"intercept\":True, \"regType\":\"l2\"}), \n",
    "                                (LogisticRegressionWithSGD, {\"intercept\":True, \"regType\":None}), \n",
    "                                (LogisticRegressionWithSGD, {\"intercept\":True, \"regType\":\"l1\"}), \n",
    "                                (LogisticRegressionWithSGD, {\"intercept\":True, \"regType\":\"l2\"})\n",
    "                           ]\n",
    "    for modelClass, kwargs in classificationModels:\n",
    "        trainClassificationModelWithROC(sc, trainingData, validationData, modelClass, **kwargs)\n",
    "    classificationModels = [\n",
    "                                (DecisionTree, {\"numClasses\":2,\"categoricalFeaturesInfo\":{},\"minInstancesPerNode\":100 ,\"impurity\":\"gini\"}), \n",
    "                                (DecisionTree, {\"numClasses\":2,\"categoricalFeaturesInfo\":{},\"minInstancesPerNode\":100 , \"impurity\":\"entropy\"}), \n",
    "                                (RandomForest, {\"numClasses\":2,\"categoricalFeaturesInfo\":{},\"numTrees\":20, \"impurity\":\"gini\"}), \n",
    "                                (RandomForest, {\"numClasses\":2,\"categoricalFeaturesInfo\":{},\"numTrees\":20, \"impurity\":\"entropy\"}) \n",
    "                           ]\n",
    "    for modelClass, kwargs in classificationModels:\n",
    "        trainClassificationModelNoROC(sc, trainingData, validationData, modelClass, **kwargs)\n",
    "\n",
    "def trainClassificationModelWithROC(sc, trainingData, validationData, modelClass, **kwargs):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    print \"Classification Model: %s %s\" % (modelClass.__name__, kwargs)\n",
    "    startTime = time.time()\n",
    "    trainingData = trainingData \\\n",
    "                    .map(lambda lp:LabeledPoint(1, lp.features) if lp.label>=1965 else LabeledPoint(0, lp.features))\n",
    "    validationData = validationData \\\n",
    "                    .map(lambda lp:LabeledPoint(1, lp.features) if lp.label>=1965 else LabeledPoint(0, lp.features))\n",
    "    model = modelClass.train(trainingData, **kwargs)\n",
    "    model.clearThreshold()\n",
    "    validationsResult = validationData.map(lambda lp:(float(model.predict(lp.features)), lp.label))\n",
    "    metric = BinaryClassificationMetrics(validationsResult)\n",
    "    errors = []\n",
    "    for i in range(1, 11):\n",
    "        err = validationsResult.filter(lambda (predict,label):(1 if predict>i/10.0 else 0)!=label).count() \\\n",
    "                                            / float(validationsResult.count())\n",
    "        errors.append((err, i/10.0))\n",
    "    errors.sort(key=lambda t:t[0])\n",
    "    print \"[ Error: %.4f\\t\\tPrecision-recall: %.4f\\tROC: %.4f ] - %s sec\" \\\n",
    "            % (errors[0][0], metric.areaUnderPR, metric.areaUnderROC, (time.time()-startTime))\n",
    "        \n",
    "def trainClassificationModelNoROC(sc, trainingData, validationData, modelClass, **kwargs):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    print \"Classification Model: %s %s\" % (modelClass.__name__, kwargs)\n",
    "    startTime = time.time()\n",
    "    trainingData = trainingData \\\n",
    "                    .map(lambda lp:LabeledPoint(1, lp.features) if lp.label>=1965 else LabeledPoint(0, lp.features))\n",
    "    validationData = validationData \\\n",
    "                    .map(lambda lp:LabeledPoint(1, lp.features) if lp.label>=1965 else LabeledPoint(0, lp.features))\n",
    "    model = modelClass.trainClassifier(trainingData, **kwargs)\n",
    "    validationFeatures = validationData.map(lambda lp:lp.features)\n",
    "    validationsResult = model.predict(validationFeatures)\n",
    "    totalCount = validationsResult.count()\n",
    "    validationsResult = validationsResult.zip(validationData.map(lambda lp:lp.label))\n",
    "    errCount = validationsResult.filter(lambda (predict,label):predict!=label).count()\n",
    "    validationsResult = validationsResult.zip(validationData.map(lambda lp:lp.label))\n",
    "    err = float(errCount) / totalCount\n",
    "    print \"[ Error: %.4f ] - %s sec\" % (err, (time.time()-startTime))\n",
    "        \n",
    "if __name__==\"__main__\":\n",
    "    filePath = \"file:///ipython/YearPredictionMSD.txt\"\n",
    "    sc = SparkContext(appName=\"MainContext\")\n",
    "    rawData = importRawData(sc, filePath).cache()\n",
    "    try:\n",
    "        selection = exploreData(rawData)\n",
    "        trainingData, validationData = featureEngineering(rawData, selection = selection)\n",
    "        selectClassificationModel(sc, trainingData, validationData)\n",
    "    except Exception:\n",
    "        raise\n",
    "    finally:\n",
    "        sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
