{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================\n",
      "EXPLORE DATA\n",
      "==============================================\n",
      "Features:\n",
      "Format:\t\t[      mean\t    variance\tnumNonzeors\t         max\t       min]\n",
      "Feature 2:\t[      43.39\t       36.82\t    515345\t       61.97\t      1.75 ] \n",
      "Feature 3:\t[       1.29\t     2660.53\t    515345\t      384.07\t   -337.09 ] \n",
      "Feature 4:\t[       8.66\t     1243.87\t    515345\t      322.85\t   -301.01 ] \n",
      "Feature 5:\t[       1.16\t      266.43\t    515345\t      335.77\t   -154.18 ] \n",
      "Feature 6:\t[      -6.55\t      522.62\t    515345\t      262.07\t   -181.95 ] \n",
      "Feature 7:\t[      -9.52\t      165.32\t    515345\t      166.24\t    -81.79 ] \n",
      "Feature 8:\t[      -2.39\t      212.34\t    515345\t      172.40\t   -188.21 ] \n",
      "Feature 9:\t[      -1.79\t       63.42\t    515345\t      126.74\t    -72.50 ] \n",
      "Feature 10:\t[       3.73\t      112.00\t    515345\t      146.30\t   -126.48 ] \n",
      "Feature 11:\t[       1.88\t       42.64\t    515345\t       60.35\t    -41.63 ] \n",
      "Feature 12:\t[      -0.15\t       19.10\t    515343\t       88.02\t    -69.68 ] \n",
      "Feature 13:\t[       2.55\t       69.23\t    515345\t       87.91\t    -94.04 ] \n",
      "Feature 14:\t[      33.71\t      495.49\t    515345\t      549.76\t      0.13 ] \n",
      "Feature 15:\t[    2439.36\t  3060287.31\t    515345\t    65735.78\t      8.47 ] \n",
      "Feature 16:\t[    1967.73\t  1591343.88\t    515345\t    36816.79\t     21.21 ] \n",
      "Feature 17:\t[    1514.86\t  1194279.69\t    515345\t    31849.49\t     17.86 ] \n",
      "Feature 18:\t[     910.98\t   226298.29\t    515345\t    19865.93\t     12.15 ] \n",
      "Feature 19:\t[     879.15\t   332774.37\t    515345\t    16831.95\t      5.52 ] \n",
      "Feature 20:\t[     603.74\t   100805.80\t    515345\t    11901.71\t     19.81 ] \n",
      "Feature 21:\t[     517.58\t    95706.54\t    515345\t     9569.78\t      6.25 ] \n",
      "Feature 22:\t[     393.96\t    45801.78\t    515345\t     9616.62\t      6.18 ] \n",
      "Feature 23:\t[     325.73\t    27456.29\t    515345\t     3721.87\t     15.31 ] \n",
      "Feature 24:\t[     288.89\t    34954.31\t    515345\t     6737.12\t      6.12 ] \n",
      "Feature 25:\t[     291.97\t    23554.77\t    515345\t     9813.23\t      5.18 ] \n",
      "Feature 26:\t[      43.03\t    14745.12\t    515345\t     2049.60\t  -2821.43 ] \n",
      "Feature 27:\t[      43.31\t   512311.16\t    515345\t    24479.66\t -13390.36 ] \n",
      "Feature 28:\t[     -46.45\t   296894.07\t    515345\t    14505.34\t -12017.09 ] \n",
      "Feature 29:\t[     -27.67\t    47684.79\t    515345\t     3410.62\t  -4324.86 ] \n",
      "Feature 30:\t[      14.96\t    26639.39\t    515345\t     3277.63\t  -3357.28 ] \n",
      "Feature 31:\t[      44.51\t    18194.73\t    515345\t     3553.18\t  -3115.37 ] \n",
      "Feature 32:\t[       5.13\t     9818.46\t    515345\t     2347.41\t  -3805.67 ] \n",
      "Feature 33:\t[      24.03\t     5156.67\t    515345\t     1954.36\t  -1516.36 ] \n",
      "Feature 34:\t[       9.50\t     5537.10\t    515345\t     2887.85\t  -1679.12 ] \n",
      "Feature 35:\t[      -4.18\t     2864.07\t    515345\t     2330.33\t  -1590.64 ] \n",
      "Feature 36:\t[       0.50\t     1805.43\t    515345\t     1813.24\t   -989.65 ] \n",
      "Feature 37:\t[      72.65\t    11645.14\t    515345\t     2496.12\t  -1711.48 ] \n",
      "Feature 38:\t[     -51.44\t   172257.45\t    515345\t    14149.00\t  -8448.19 ] \n",
      "Feature 39:\t[     117.92\t   205570.86\t    515345\t     8059.15\t -10095.73 ] \n",
      "Feature 40:\t[    -189.88\t    67936.01\t    515345\t     6065.05\t  -9803.76 ] \n",
      "Feature 41:\t[      23.10\t    42328.78\t    515345\t     8360.15\t  -7882.82 ] \n",
      "Feature 42:\t[      -1.28\t    14359.92\t    515345\t     3537.50\t  -4673.36 ] \n",
      "Feature 43:\t[      18.15\t    14363.00\t    515345\t     3892.12\t  -4175.41 ] \n",
      "Feature 44:\t[     -51.96\t     5420.26\t    515345\t     1202.49\t  -4975.38 ] \n",
      "Feature 45:\t[       3.23\t     1472.55\t    515345\t     1830.54\t  -1072.96 ] \n",
      "Feature 46:\t[      -1.49\t     1726.01\t    515345\t      746.71\t  -1021.29 ] \n",
      "Feature 47:\t[       6.33\t     3022.18\t    515345\t     1198.63\t  -1329.96 ] \n",
      "Feature 48:\t[      78.70\t   221461.91\t    515345\t     9059.76\t -14861.70 ] \n",
      "Feature 49:\t[     142.70\t    68838.05\t    515345\t     6967.64\t  -3992.69 ] \n",
      "Feature 50:\t[     -86.52\t    43904.14\t    515345\t     6172.35\t  -6642.40 ] \n",
      "Feature 51:\t[      25.24\t    14921.51\t    515345\t     2067.20\t  -2344.53 ] \n",
      "Feature 52:\t[       6.38\t     8752.66\t    515345\t     1426.85\t  -2270.81 ] \n",
      "Feature 53:\t[      28.29\t     5632.39\t    515345\t     2460.43\t  -1746.48 ] \n",
      "Feature 54:\t[      12.77\t     4897.46\t    515345\t     2394.66\t  -3188.18 ] \n",
      "Feature 55:\t[       1.70\t     6935.34\t    515345\t     2900.52\t  -2199.78 ] \n",
      "Feature 56:\t[     -10.21\t     3341.60\t    515345\t      569.14\t  -1694.26 ] \n",
      "Feature 57:\t[      64.10\t    74981.33\t    515345\t     6955.41\t  -5154.02 ] \n",
      "Feature 58:\t[     104.82\t    96805.17\t    515345\t    12700.01\t  -5111.60 ] \n",
      "Feature 59:\t[      -0.03\t    71168.38\t    515345\t    13001.26\t  -4730.60 ] \n",
      "Feature 60:\t[      38.68\t    28585.40\t    515345\t     5419.28\t  -3756.49 ] \n",
      "Feature 61:\t[     -27.99\t    20759.92\t    515345\t     5690.29\t  -2499.95 ] \n",
      "Feature 62:\t[       3.30\t     3533.97\t    515345\t     1811.23\t  -1900.10 ] \n",
      "Feature 63:\t[       0.31\t     2418.81\t    515345\t      973.05\t  -1396.70 ] \n",
      "Feature 64:\t[      -0.48\t     1419.44\t    515344\t      812.42\t   -600.09 ] \n",
      "Feature 65:\t[    -138.22\t    94979.47\t    515345\t    11048.20\t -10345.83 ] \n",
      "Feature 66:\t[      -0.70\t    49372.18\t    515345\t     2877.74\t  -7375.98 ] \n",
      "Feature 67:\t[       0.24\t    16420.11\t    515345\t     3447.48\t  -3896.28 ] \n",
      "Feature 68:\t[       3.15\t     9984.20\t    515345\t     2055.04\t  -1199.00 ] \n",
      "Feature 69:\t[      27.64\t    13618.00\t    515345\t     4779.80\t  -2564.79 ] \n",
      "Feature 70:\t[      31.82\t    11312.44\t    515345\t     5286.82\t  -1904.98 ] \n",
      "Feature 71:\t[      -0.84\t     1354.18\t    515345\t      745.50\t   -974.70 ] \n",
      "Feature 72:\t[      -8.93\t    63305.96\t    515345\t     3958.07\t  -7057.71 ] \n",
      "Feature 73:\t[       4.85\t    52468.68\t    515345\t     4741.18\t  -6953.36 ] \n",
      "Feature 74:\t[     -27.35\t    26824.75\t    515345\t     2124.10\t  -8400.60 ] \n",
      "Feature 75:\t[     -11.94\t     4005.48\t    515344\t     1639.93\t  -1812.89 ] \n",
      "Feature 76:\t[     -21.57\t     4185.00\t    515345\t     1278.33\t  -1387.51 ] \n",
      "Feature 77:\t[      -5.58\t      694.75\t    515345\t      741.03\t   -718.42 ] \n",
      "Feature 78:\t[     -23.30\t    71926.37\t    515345\t    10020.28\t  -9831.45 ] \n",
      "Feature 79:\t[      31.11\t    20781.01\t    515345\t     3423.60\t  -2025.78 ] \n",
      "Feature 80:\t[    -104.97\t    40456.30\t    515345\t     5188.33\t  -8390.04 ] \n",
      "Feature 81:\t[      26.96\t    15338.30\t    515345\t     3735.03\t  -4754.94 ] \n",
      "Feature 82:\t[      15.76\t     1030.39\t    515345\t      840.97\t   -437.72 ] \n",
      "Feature 83:\t[     -73.46\t    30841.99\t    515345\t     4469.45\t  -4402.38 ] \n",
      "Feature 84:\t[      41.54\t    14939.88\t    515345\t     3210.70\t  -1810.69 ] \n",
      "Feature 85:\t[      37.93\t     9034.62\t    515345\t     1734.08\t  -3098.35 ] \n",
      "Feature 86:\t[       0.32\t      261.20\t    515345\t      260.54\t   -341.79 ] \n",
      "Feature 87:\t[      17.67\t    13093.75\t    515345\t     3662.07\t  -3168.92 ] \n",
      "Feature 88:\t[     -26.32\t    30268.11\t    515345\t     2833.61\t  -4319.99 ] \n",
      "Feature 89:\t[       4.46\t      178.13\t    515345\t      463.42\t   -236.04 ] \n",
      "Feature 90:\t[      20.04\t    34431.86\t    515345\t     7393.40\t  -7458.38 ] \n",
      "Feature 91:\t[       1.33\t      487.91\t    515345\t      677.90\t   -381.42 ] \n",
      "Label:\n",
      "Label :\t[    1998.40\t      119.49\t    515345\t     2011.00\t   1922.00 ] \n",
      "Correlation between Features and Label (only show corr > 0.05): \n",
      "Feature 1:  0.2254\n",
      "Feature 3: -0.1874\n",
      "Feature 2: -0.1395\n",
      "Feature 27: -0.1263\n",
      "Feature 17: -0.1240\n",
      "Feature 4:  0.1105\n",
      "Feature 29: -0.1028\n",
      "Feature 19: -0.1013\n",
      "Feature 14: -0.1010\n",
      "Feature 31: -0.0995\n",
      "Feature 5: -0.0972\n",
      "Feature 24: -0.0970\n",
      "Feature 20:  0.0961\n",
      "Feature 25: -0.0927\n",
      "Feature 13: -0.0912\n",
      "Feature 35: -0.0874\n",
      "Feature 7:  0.0871\n",
      "Feature 33:  0.0838\n",
      "Feature 34:  0.0817\n",
      "Feature 30:  0.0813\n",
      "Feature 6:  0.0787\n",
      "Feature 15:  0.0744\n",
      "Feature 23:  0.0741\n",
      "Feature 12: -0.0739\n",
      "Feature 11:  0.0733\n",
      "Feature 16:  0.0703\n",
      "Feature 32: -0.0683\n",
      "Feature 26: -0.0672\n",
      "Feature 10:  0.0647\n",
      "Feature 21: -0.0629\n",
      "Feature 18: -0.0608\n",
      "Feature 36:  0.0595\n",
      "Feature 9: -0.0582\n",
      "Feature 8: -0.0567\n",
      "Feature 38: -0.0565\n",
      "Feature 28: -0.0544\n",
      "Feature 37: -0.0535\n",
      "Feature 22: -0.0531\n",
      "Correlation between Features (only show colinear > 0.8): \n",
      "Feature 16 and Feature 18 have r=0.8096\n",
      "Feature 16 and Feature 23 have r=0.8466\n",
      "Feature 18 and Feature 23 have r=0.8596\n",
      "Feature 20 and Feature 22 have r=0.8657\n",
      "==============================================\n",
      "FEATURE ENGINEERING\n",
      "==============================================\n",
      "partitioning...\n",
      "Normalization and Scaling... \n",
      "Feature Selection... \n",
      "Greedy Correlation Feature Selection (CFS)\n",
      "k=1\tHighest Merit=0.1594\tSelected:[1]\n",
      "k=2\tHighest Merit=0.2195\tSelected:[1, 6]\n",
      "k=3\tHighest Merit=0.2444\tSelected:[1, 6, 3]\n",
      "k=4\tHighest Merit=0.2591\tSelected:[1, 6, 3, 40]\n",
      "k=5\tHighest Merit=0.2639\tSelected:[1, 6, 3, 40, 12]\n",
      "k=6\tHighest Merit=0.2680\tSelected:[1, 6, 3, 40, 12, 47]\n",
      "k=7\tHighest Merit=0.2715\tSelected:[1, 6, 3, 40, 12, 47, 74]\n",
      "k=8\tHighest Merit=0.2734\tSelected:[1, 6, 3, 40, 12, 47, 74, 78]\n",
      "k=9\tHighest Merit=0.2749\tSelected:[1, 6, 3, 40, 12, 47, 74, 78, 63]\n",
      "k=10\tHighest Merit=0.2772\tSelected:[1, 6, 3, 40, 12, 47, 74, 78, 63, 69]\n",
      "k=11\tHighest Merit=0.2790\tSelected:[1, 6, 3, 40, 12, 47, 74, 78, 63, 69, 25]\n",
      "k=12\tHighest Merit=0.2800\tSelected:[1, 6, 3, 40, 12, 47, 74, 78, 63, 69, 25, 53]\n",
      "k=13\tHighest Merit=0.2810\tSelected:[1, 6, 3, 40, 12, 47, 74, 78, 63, 69, 25, 53, 7]\n",
      "Selected 13 Features: \n",
      "Dimension Reduction...\n",
      "PCA Eigen Vector: [0.031796664566865314, 0.019428187278227014, 0.015615472371740229, 0.013772762697838255, 0.012949690108360976, 0.011673650201902436, 0.01062035754085058, 0.0099405563729845943, 0.0094815231200014536, 0.0083550918089512448, 0.0079250516980137065, 0.0064260689682527046, 0.0057990573183351081]\n",
      "Accumulate Variance Porpotion:\n",
      "1 Dimensions:  19.41%\n",
      "2 Dimensions:  31.28%\n",
      "3 Dimensions:  40.81%\n",
      "4 Dimensions:  49.22%\n",
      "5 Dimensions:  57.13%\n",
      "6 Dimensions:  64.25%\n",
      "7 Dimensions:  70.74%\n",
      "8 Dimensions:  76.81%\n",
      "9 Dimensions:  82.60%\n",
      "10 Dimensions:  87.70%\n",
      "11 Dimensions:  92.54%\n",
      "12 Dimensions:  96.46%\n",
      "13 Dimensions:  100.00%\n",
      "Reduce to 11 dimensions...\n",
      "=============================================="
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS,SVMWithSGD,LogisticRegressionWithSGD, NaiveBayes\n",
    "from pyspark.mllib.regression import LabeledPoint,LinearRegressionWithSGD,RidgeRegressionWithSGD,LassoWithSGD\n",
    "from pyspark.mllib.tree import DecisionTree, RandomForest, GradientBoostedTrees\n",
    "from pyspark.mllib.feature import StandardScaler,ChiSqSelector,Normalizer,PCA\n",
    "from pyspark.mllib.evaluation import RegressionMetrics, BinaryClassificationMetrics\n",
    "from pyspark.mllib.stat import Statistics, MultivariateStatisticalSummary\n",
    "from pyspark.mllib.util import MLUtils\n",
    "import time\n",
    "\n",
    "def importRawData(sc, filePath):\n",
    "    \"\"\"\n",
    "    :param sc: Spark Context\n",
    "    :param filePath: path to data .csv file\n",
    "    :return: RDD of (LabeledPoint, index)\n",
    "    \"\"\"\n",
    "    rdd = sc.textFile(filePath)\n",
    "    # index was keep for this dataset, we need index to perform partition\n",
    "    # spark DOES NOT gurantee tranformation and action execute order for RDD[Vector]\n",
    "    return rdd.map(lambda line: line.split(\",\")) \\\n",
    "                .map(lambda array: [float(n) for n in array]) \\\n",
    "                .zipWithIndex()\n",
    "\n",
    "def exploreData(rawData, corrThreshold = 0.05, colinearThreshold = 0.8):\n",
    "    \"\"\"\n",
    "    this function is use to print out some basic statistic of data\n",
    "    :param rawData: RDD[Vector] of raw data\n",
    "    :param corrThreshold: correlation threshold for print\n",
    "    :param colinearThreshold: colinear threshold for print\n",
    "    :return: local matrix of correliation matrix, corr[0] as label, rest are features\n",
    "    \"\"\"\n",
    "    print \"==============================================\"\n",
    "    print \"EXPLORE DATA\"\n",
    "    print \"==============================================\"\n",
    "    matrix = rawData.map(lambda t:t[0])\n",
    "    trait = Statistics.colStats(matrix)\n",
    "    print \"Features:\"\n",
    "    print \"Format:\\t\\t[% 10s\\t% 12s\\t% 10s\\t% 12s\\t% 10s]\" % (\"mean\",\"variance\",\"numNonzeors\",\"max\",\"min\")\n",
    "    for i in range(1, len(trait.mean())):\n",
    "        print \"Feature %s:\\t[ % 10.2f\\t% 12.2f\\t% 10d\\t% 12.2f\\t% 10.2f ] \" % (i+1, trait.mean()[i], trait.variance()[i],\\\n",
    "                                                    trait.numNonzeros()[i], trait.max()[i], trait.min()[i])\n",
    "    print \"Label:\"\n",
    "    print \"Label :\\t[ % 10.2f\\t% 12.2f\\t% 10d\\t% 12.2f\\t% 10.2f ] \" % (trait.mean()[0], trait.variance()[0], \\\n",
    "                                            trait.numNonzeros()[0], trait.max()[0], trait.min()[0])\n",
    "    corr = Statistics.corr(matrix, method=\"pearson\")\n",
    "    print \"Correlation between Features and Label (only show corr > %s): \" % corrThreshold\n",
    "    labelCorr = zip([row[0] for row in corr[1:] if abs(row[0])>=corrThreshold], range(1, len(corr)))\n",
    "    labelCorr.sort(key=lambda t:abs(t[0]), reverse=True)\n",
    "    for corrScore, i in labelCorr:\n",
    "        print \"Feature %s: % 6.4f\" % (i, corrScore)\n",
    "    print \"Correlation between Features (only show colinear > %s): \" % colinearThreshold\n",
    "    for i, row in enumerate(corr):\n",
    "        for j, r in enumerate(row):\n",
    "            if (i!=0 and j!=0 and i<j and abs(r)>colinearThreshold):\n",
    "                print \"Feature %s and Feature %s have r=%.4f\" % (i, j, r)\n",
    "    return corr\n",
    "\n",
    "def featureEngineering(rawData, corrSelection = None, zNorm = True, l2Norm = True, categorical = False, topFeature = 10):\n",
    "    \"\"\"\n",
    "    this function is to provide feature engineering and transformation\n",
    "    including partition, normalization, feature selection, dimension reduction\n",
    "    :param rawData: RDD[Vector] of raw data\n",
    "    :param corrSelection: local matrix of correliation matrix, if provided will perform CFS\n",
    "    :param zNorm: boolean of whether to perform Z normalization or not\n",
    "    :param l2Norm: boolean of whether to perform L2 normalization or not\n",
    "    :param categorical: boolean of whether to perform chi square feature selection or not\n",
    "    :param topFeature: select top features if using chi square selection\n",
    "    :return: tuple of (RDD[LabeledPoint] trainingData, RDD[LabeledPoint] validationData)\n",
    "    \"\"\"\n",
    "    print \"==============================================\"\n",
    "    print \"FEATURE ENGINEERING\"\n",
    "    print \"==============================================\"\n",
    "    print \"partitioning...\"\n",
    "    # beware this partitioning is HARD CODED!\n",
    "    # because it was suggested by data set creator\n",
    "    # first 463715 used as trainning, last 51630 used as validation\n",
    "    tFeatures = rawData.filter(lambda a:a[1]<463715).map(lambda a:a[0][1:])\n",
    "    vFeatures = rawData.filter(lambda a:a[1]>=463715).map(lambda a:a[0][1:])\n",
    "    tLabel = rawData.filter(lambda a:a[1]<463715).map(lambda a:a[0][0])\n",
    "    vLabel = rawData.filter(lambda a:a[1]>=463715).map(lambda a:a[0][0])\n",
    "    print \"Normalization and Scaling... \"\n",
    "    if(zNorm):\n",
    "        zN = StandardScaler(withMean=True, withStd=True).fit(tFeatures)\n",
    "        tFeatures = zN.transform(tFeatures)\n",
    "        vFeatures = zN.transform(vFeatures)\n",
    "    if(l2Norm):\n",
    "        l2N = Normalizer()\n",
    "        tFeatures = l2N.transform(tFeatures)\n",
    "        vFeatures = l2N.transform(vFeatures)\n",
    "    print \"Feature Selection... \"\n",
    "    # only categorical value for classification problem could use chi square selector\n",
    "    # otherwise, use correlation based selector instead\n",
    "    if categorical or (corrSelection is None):\n",
    "        selector = ChiSqSelector(topFeature).fit(tFeatures)\n",
    "        tFeatures = selector.transform(tFeatures)\n",
    "        vFeatures = selector.transform(vFeatures)\n",
    "    else:\n",
    "        bestFeatures = cfs(corrSelection)\n",
    "        tFeatures = tFeatures.map(lambda a:[a[n-1] for n in bestFeatures])\n",
    "        vFeatures = vFeatures.map(lambda a:[a[n-1] for n in bestFeatures])\n",
    "    featureCount = len(tFeatures.first())\n",
    "    print \"Selected %s Features: \" % featureCount\n",
    "    print \"Dimension Reduction...\"\n",
    "    # PCA dimension reduction = EVD of cov matrix = SVD of data matrix\n",
    "    # spark use EVD implement, hence we need to calculate eigen vector ourself to decide cutoff\n",
    "    # cutoff point the reduced dimension could represent 90% of original total variance\n",
    "    selector = PCA(len(bestFeatures)).fit(tFeatures.map(lambda a: LabeledPoint(1,a).features))\n",
    "    temp = selector.transform(tFeatures)\n",
    "    eigen = eigenvalues(temp)\n",
    "    print \"PCA Eigen Vector: %s\" % eigen\n",
    "    accVarPor = [sum(eigen[:i+1])/sum(eigen) for i in range(len(eigen))]\n",
    "    print \"Accumulate Variance Porpotion:\"\n",
    "    reduction = 0\n",
    "    for i in range(len(accVarPor)):\n",
    "        print \"%s Dimensions:  %.2f%%\" % (i+1, accVarPor[i]*100)\n",
    "        if(reduction==0 and accVarPor[i]>0.9): reduction = i+1\n",
    "    print \"Reduce to %s dimensions...\" % reduction\n",
    "    selector = PCA(reduction).fit(tFeatures.map(lambda a: LabeledPoint(1,a).features))\n",
    "    return (tLabel.zip(tFeatures).map(lambda lp:LabeledPoint(lp[0], lp[1])).cache(), \\\n",
    "            vLabel.zip(vFeatures).map(lambda lp:LabeledPoint(lp[0], lp[1])).cache())\n",
    "\n",
    "def cfs(corr):\n",
    "    \"\"\"\n",
    "    helper function of correlation feature selection, implemented in greedy algorithm O(n^2logn)\n",
    "    :param corr: local matrix of correliation matrix\n",
    "    :return: local list of best k features, list element as feature index (1 based)\n",
    "    \"\"\"\n",
    "    features = range(1,len(corr))\n",
    "    bestK = []\n",
    "    queue = []\n",
    "    merit = -1\n",
    "    sumL = 0\n",
    "    sumF = 0\n",
    "    print \"Greedy Correlation Feature Selection (CFS)\"\n",
    "    # the stop condition of this algorithm is we reach maximum merit\n",
    "    # i.e. merit start to decrease as we include more features\n",
    "    # because of the increase correlation between features-to-features outweight featur-to-labels'\n",
    "    # that's a bad thing to avoid, hence we stop there\n",
    "    while(len(features)>0):\n",
    "        for f in features:\n",
    "            bestK.append(f)\n",
    "            tempL = sumL+abs(corr[0][f])\n",
    "            tempF = sumF\n",
    "            for cur in bestK:\n",
    "                if cur!=f:\n",
    "                    tempF += abs(corr[cur][f])\n",
    "            queue.append((tempL/((len(bestK)+1+2*tempF)**0.5), f, tempL, tempF))\n",
    "            bestK.remove(f)\n",
    "        queue.sort(key=lambda e:e[0], reverse=True)\n",
    "        if(queue[0][0]<=merit):\n",
    "            break\n",
    "        merit = queue[0][0]\n",
    "        bestK.append(queue[0][1])\n",
    "        features.remove(queue[0][1])\n",
    "        sumL = queue[0][2]\n",
    "        sumF = queue[0][3]\n",
    "        print \"k=%s\\tHighest Merit=%.4f\\tSelected:%s\" % (len(bestK),merit,bestK)\n",
    "    return bestK\n",
    "\n",
    "def eigenvalues(data):\n",
    "    \"\"\"\n",
    "    helper function of calculate eigen vector of PCA, to determine how many dimension after reduction\n",
    "    :param data: RDD[Vector] of data after PCA transformation\n",
    "    :return: local list of eigen vector based on input data\n",
    "    \"\"\"\n",
    "    # only eigen value is calculated, not the whole covariance because that would be too slow\n",
    "    count = data.count()\n",
    "    miu = data.reduce(lambda a,b:[a[i]+b[i] for i in range(len(a))])\n",
    "    miu = map(lambda s:s/count, miu)\n",
    "    eigen = data.map(lambda a:[(a[i]-miu[i])**2 for i in range(len(a))]) \\\n",
    "                .reduce(lambda a,b:[a[i]+b[i] for i in range(len(a))])\n",
    "    eigen = map(lambda s:s/count, eigen)\n",
    "    return eigen\n",
    "\n",
    "def selectClassificationModel(sc, trainingData, validationData):\n",
    "    \"\"\"\n",
    "    wrapper function to evaluate and select all the classification models\n",
    "    :param sc: spark context\n",
    "    :param trainingData: RDD[LabeledPoint] of training data\n",
    "    :param validationData: RDD[LabeledPoint] of validation data\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    print \"==============================================\"\n",
    "    print \"CLASSIFICATION\"\n",
    "    print \"==============================================\"\n",
    "    classificationModels = [\n",
    "                                (SVMWithSGD, {\"intercept\":True, \"regType\":None}), \n",
    "                                (SVMWithSGD, {\"intercept\":True, \"regType\":\"l1\"}), \n",
    "                                (SVMWithSGD, {\"intercept\":True, \"regType\":\"l2\"}), \n",
    "                                (LogisticRegressionWithLBFGS, {\"intercept\":True, \"regType\":\"l1\"}), \n",
    "                                (LogisticRegressionWithLBFGS, {\"intercept\":True, \"regType\":\"l2\"}), \n",
    "                                (LogisticRegressionWithSGD, {\"intercept\":True, \"regType\":None}), \n",
    "                                (LogisticRegressionWithSGD, {\"intercept\":True, \"regType\":\"l1\"}), \n",
    "                                (LogisticRegressionWithSGD, {\"intercept\":True, \"regType\":\"l2\"})\n",
    "                           ]\n",
    "    for modelClass, kwargs in classificationModels:\n",
    "        trainClassificationModel(sc, trainingData, validationData, modelClass, **kwargs)\n",
    "    #GBT is waaaaaay too slow for this dataset\n",
    "    classificationModels = [\n",
    "                                (DecisionTree, {\"numClasses\":2,\"categoricalFeaturesInfo\":{},\"minInstancesPerNode\":100 ,\"impurity\":\"gini\"}), \n",
    "                                (DecisionTree, {\"numClasses\":2,\"categoricalFeaturesInfo\":{},\"minInstancesPerNode\":100 , \"impurity\":\"entropy\"}), \n",
    "                                (RandomForest, {\"numClasses\":2,\"categoricalFeaturesInfo\":{},\"numTrees\":20, \"impurity\":\"gini\"}), \n",
    "                                (RandomForest, {\"numClasses\":2,\"categoricalFeaturesInfo\":{},\"numTrees\":20, \"impurity\":\"entropy\"}) \n",
    "                           ]\n",
    "    for modelClass, kwargs in classificationModels:\n",
    "        trainClassificationTreeModel(sc, trainingData, validationData, modelClass, **kwargs)\n",
    "\n",
    "def trainClassificationModel(sc, trainingData, validationData, modelClass, **kwargs):\n",
    "    \"\"\"\n",
    "    train classification models for NOT-TREE based model\n",
    "    :param sc: spark context\n",
    "    :param trainingData: RDD[LabeledPoint] of training data\n",
    "    :param validationData: RDD[LabeledPoint] of validation data\n",
    "    :modelClass: model CLASS that use to train\n",
    "    :kwargs: key-value paired arguments for modelClass, would be passes in directly\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    print \"Classification Model: %s %s\" % (modelClass.__name__, kwargs)\n",
    "    startTime = time.time()\n",
    "    trainingData = trainingData \\\n",
    "                    .map(lambda lp:LabeledPoint(1, lp.features) if lp.label>=2000 else LabeledPoint(0, lp.features))\n",
    "    validationData = validationData \\\n",
    "                    .map(lambda lp:LabeledPoint(1, lp.features) if lp.label>=2000 else LabeledPoint(0, lp.features))\n",
    "    model = modelClass.train(trainingData, **kwargs)\n",
    "    model.clearThreshold()\n",
    "    validationsResult = validationData.map(lambda lp:(float(model.predict(lp.features)), lp.label))\n",
    "    metric = BinaryClassificationMetrics(validationsResult)\n",
    "    # the error rate search is to search for overall best error rate\n",
    "    # regardless of precision and recall, however they could be evaluate by PR area and ROC area\n",
    "    errors = []\n",
    "    for i in range(1, 11):\n",
    "        err = validationsResult.filter(lambda (predict,label):(1 if predict>i/10.0 else 0)!=label).count() \\\n",
    "                                            / float(validationsResult.count())\n",
    "        errors.append((err, i/10.0))\n",
    "    errors.sort(key=lambda t:t[0])\n",
    "    print \"[ Error: %.4f\\t\\tPrecision-recall: %.4f\\tROC: %.4f ] - %s sec\" \\\n",
    "            % (errors[0][0], metric.areaUnderPR, metric.areaUnderROC, (time.time()-startTime))\n",
    "        \n",
    "def trainClassificationTreeModel(sc, trainingData, validationData, modelClass, **kwargs):\n",
    "    \"\"\"\n",
    "    train classification models for TREE based model\n",
    "    :param sc: spark context\n",
    "    :param trainingData: RDD[LabeledPoint] of training data\n",
    "    :param validationData: RDD[LabeledPoint] of validation data\n",
    "    :modelClass: model CLASS that use to train\n",
    "    :kwargs: key-value paired arguments for modelClass, would be passes in directly\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    print \"Classification Model: %s %s\" % (modelClass.__name__, kwargs)\n",
    "    startTime = time.time()\n",
    "    trainingData = trainingData \\\n",
    "                    .map(lambda lp:LabeledPoint(1, lp.features) if lp.label>=2000 else LabeledPoint(0, lp.features))\n",
    "    validationData = validationData \\\n",
    "                    .map(lambda lp:LabeledPoint(1, lp.features) if lp.label>=2000 else LabeledPoint(0, lp.features))\n",
    "    model = modelClass.trainClassifier(trainingData, **kwargs)\n",
    "    validationFeatures = validationData.map(lambda lp:lp.features)\n",
    "    # !!!beware, due to some stange bug, DO NOT chain RDD transformation on tree model predict, count() immediately!!!\n",
    "    validationsResult = model.predict(validationFeatures)\n",
    "    totalCount = validationsResult.count()\n",
    "    validationsResult = validationsResult.zip(validationData.map(lambda lp:lp.label))\n",
    "    errCount = validationsResult.filter(lambda (predict,label):predict!=label).count()\n",
    "    validationsResult = validationsResult.zip(validationData.map(lambda lp:lp.label))\n",
    "    err = float(errCount) / totalCount\n",
    "    print \"[ Error: %.4f ] - %s sec\" % (err, (time.time()-startTime))\n",
    "    \n",
    "def selectRegressionModel(sc, trainingData, validationData):\n",
    "    \"\"\"\n",
    "    wrapper function to evaluate and select all the regression models\n",
    "    :param sc: spark context\n",
    "    :param trainingData: RDD[LabeledPoint] of training data\n",
    "    :param validationData: RDD[LabeledPoint] of validation data\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    print \"==============================================\"\n",
    "    print \"REGRESSION\"\n",
    "    print \"==============================================\"\n",
    "    regressionModels = [\n",
    "                            (LinearRegressionWithSGD, {\"intercept\":True, \"regType\":None}), \n",
    "                            (RidgeRegressionWithSGD, {\"intercept\":True}), \n",
    "                            (LassoWithSGD, {\"intercept\":True})\n",
    "                       ]\n",
    "    for modelClass, kwargs in regressionModels:\n",
    "        trainRegressionModel(sc, trainingData, validationData, modelClass, **kwargs)\n",
    "    regressionModels = [\n",
    "                            (DecisionTree, {\"categoricalFeaturesInfo\":{},\"minInstancesPerNode\":100 ,\"impurity\":\"variance\"}), \n",
    "                            (RandomForest, {\"categoricalFeaturesInfo\":{},\"numTrees\":20, \"impurity\":\"variance\"}), \n",
    "                       ]\n",
    "    for modelClass, kwargs in regressionModels:\n",
    "        trainRegressionTreeModel(sc, trainingData, validationData, modelClass, **kwargs)\n",
    "        \n",
    "def trainRegressionModel(sc, trainingData, validationData, modelClass, **kwargs):\n",
    "    \"\"\"\n",
    "    train regression models for NOT-TREE based model\n",
    "    :param sc: spark context\n",
    "    :param trainingData: RDD[LabeledPoint] of training data\n",
    "    :param validationData: RDD[LabeledPoint] of validation data\n",
    "    :modelClass: model CLASS that use to train\n",
    "    :kwargs: key-value paired arguments for modelClass, would be passes in directly\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    print \"Regression Model: %s %s\" % (modelClass.__name__, kwargs)\n",
    "    startTime = time.time()\n",
    "    model = modelClass.train(trainingData, **kwargs)\n",
    "    validationsResult = validationData.map(lambda lp:(float(model.predict(lp.features)), lp.label))\n",
    "    metric = RegressionMetrics(validationsResult)\n",
    "    print \"[ MSE: %.4f\\t\\tR^2: %.4f\\tExplained Variance: %.4f ] - %s sec\" \\\n",
    "            % (metric.meanSquaredError, metric.r2, metric.explainedVariance, (time.time()-startTime))\n",
    "        \n",
    "def trainRegressionTreeModel(sc, trainingData, validationData, modelClass, **kwargs):\n",
    "    \"\"\"\n",
    "    train regression models for TREE based model\n",
    "    :param sc: spark context\n",
    "    :param trainingData: RDD[LabeledPoint] of training data\n",
    "    :param validationData: RDD[LabeledPoint] of validation data\n",
    "    :modelClass: model CLASS that use to train\n",
    "    :kwargs: key-value paired arguments for modelClass, would be passes in directly\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    print \"Regression Model: %s %s\" % (modelClass.__name__, kwargs)\n",
    "    startTime = time.time()\n",
    "    model = modelClass.trainRegressor(trainingData, **kwargs)\n",
    "    validationFeatures = validationData.map(lambda lp:lp.features)\n",
    "    # !!!beware, due to some stange bug, DO NOT chain RDD transformation on tree model predict, count() immediately!!!\n",
    "    validationsResult = model.predict(validationFeatures)\n",
    "    validationsResult.count()\n",
    "    validationsResult = validationsResult.zip(validationData.map(lambda lp:lp.label))\n",
    "    metric = RegressionMetrics(validationsResult)\n",
    "    print \"[ MSE: %.4f\\t\\tR^2: %.4f\\tExplained Variance: %.4f ] - %s sec\" \\\n",
    "            % (metric.meanSquaredError, metric.r2, metric.explainedVariance, (time.time()-startTime))\n",
    "        \n",
    "if __name__==\"__main__\":\n",
    "    filePath = \"file:///ipython/YearPredictionMSD.txt\"\n",
    "    sc = SparkContext(appName=\"MainContext\")\n",
    "    rawData = importRawData(sc, filePath).cache()\n",
    "    try:\n",
    "        corrSelection = exploreData(rawData)\n",
    "        trainingData, validationData = featureEngineering(rawData, corrSelection = corrSelection)\n",
    "        selectClassificationModel(sc, trainingData, validationData)\n",
    "        selectRegressionModel(sc, trainingData, validationData)\n",
    "    except Exception:\n",
    "        raise\n",
    "    finally:\n",
    "        sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
