{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Path: file:///ipython/YearPredictionMSD.txt\n",
      "***********************************************\n",
      "\n",
      "Data Explore\n",
      "\n",
      "***********************************************\n",
      "Features:\n",
      "Format:\t\t[ mean\tvariance\tnumNonzeors\tmax\tmin ]\n",
      "Feature 1:\t[ 43.3871\t36.8153\t515345\t61.9701\t1.7490 ] \n",
      "Feature 2:\t[ 1.2896\t2660.5326\t515345\t384.0657\t-337.0925 ] \n",
      "Feature 3:\t[ 8.6583\t1243.8731\t515345\t322.8514\t-301.0051 ] \n",
      "Feature 4:\t[ 1.1641\t266.4335\t515345\t335.7718\t-154.1836 ] \n",
      "Feature 5:\t[ -6.5536\t522.6155\t515345\t262.0689\t-181.9534 ] \n",
      "Feature 6:\t[ -9.5220\t165.3218\t515345\t166.2369\t-81.7943 ] \n",
      "Feature 7:\t[ -2.3911\t212.3395\t515345\t172.4027\t-188.2140 ] \n",
      "Feature 8:\t[ -1.7932\t63.4225\t515345\t126.7413\t-72.5038 ] \n",
      "Feature 9:\t[ 3.7279\t111.9969\t515345\t146.2979\t-126.4790 ] \n",
      "Feature 10:\t[ 1.8824\t42.6439\t515345\t60.3454\t-41.6317 ] \n",
      "Feature 11:\t[ -0.1465\t19.1043\t515343\t88.0208\t-69.6809 ] \n",
      "Feature 12:\t[ 2.5461\t69.2256\t515345\t87.9132\t-94.0420 ] \n",
      "Feature 13:\t[ 33.7140\t495.4912\t515345\t549.7649\t0.1328 ] \n",
      "Feature 14:\t[ 2439.3594\t3060287.3135\t515345\t65735.7795\t8.4742 ] \n",
      "Feature 15:\t[ 1967.7342\t1591343.8794\t515345\t36816.7904\t21.2143 ] \n",
      "Feature 16:\t[ 1514.8599\t1194279.6907\t515345\t31849.4868\t17.8579 ] \n",
      "Feature 17:\t[ 910.9813\t226298.2946\t515345\t19865.9320\t12.1504 ] \n",
      "Feature 18:\t[ 879.1467\t332774.3749\t515345\t16831.9490\t5.5177 ] \n",
      "Feature 19:\t[ 603.7374\t100805.7978\t515345\t11901.7051\t19.8088 ] \n",
      "Feature 20:\t[ 517.5793\t95706.5390\t515345\t9569.7781\t6.2549 ] \n",
      "Feature 21:\t[ 393.9622\t45801.7789\t515345\t9616.6156\t6.1838 ] \n",
      "Feature 22:\t[ 325.7332\t27456.2854\t515345\t3721.8732\t15.3075 ] \n",
      "Feature 23:\t[ 288.8851\t34954.3141\t515345\t6737.1215\t6.1164 ] \n",
      "Feature 24:\t[ 291.9732\t23554.7748\t515345\t9813.2337\t5.1773 ] \n",
      "Feature 25:\t[ 43.0320\t14745.1230\t515345\t2049.6043\t-2821.4302 ] \n",
      "Feature 26:\t[ 43.3149\t512311.1571\t515345\t24479.6646\t-13390.3607 ] \n",
      "Feature 27:\t[ -46.4490\t296894.0675\t515345\t14505.3422\t-12017.0889 ] \n",
      "Feature 28:\t[ -27.6728\t47684.7915\t515345\t3410.6156\t-4324.8647 ] \n",
      "Feature 29:\t[ 14.9585\t26639.3937\t515345\t3277.6337\t-3357.2799 ] \n",
      "Feature 30:\t[ 44.5149\t18194.7273\t515345\t3553.1849\t-3115.3746 ] \n",
      "Feature 31:\t[ 5.1318\t9818.4554\t515345\t2347.4148\t-3805.6662 ] \n",
      "Feature 32:\t[ 24.0343\t5156.6667\t515345\t1954.3555\t-1516.3564 ] \n",
      "Feature 33:\t[ 9.4988\t5537.0981\t515345\t2887.8462\t-1679.1183 ] \n",
      "Feature 34:\t[ -4.1789\t2864.0732\t515345\t2330.3337\t-1590.6371 ] \n",
      "Feature 35:\t[ 0.4995\t1805.4337\t515345\t1813.2365\t-989.6458 ] \n",
      "Feature 36:\t[ 72.6523\t11645.1354\t515345\t2496.1226\t-1711.4840 ] \n",
      "Feature 37:\t[ -51.4413\t172257.4548\t515345\t14148.9978\t-8448.1949 ] \n",
      "Feature 38:\t[ 117.9211\t205570.8625\t515345\t8059.1465\t-10095.7260 ] \n",
      "Feature 39:\t[ -189.8809\t67936.0087\t515345\t6065.0548\t-9803.7590 ] \n",
      "Feature 40:\t[ 23.0961\t42328.7785\t515345\t8360.1456\t-7882.8232 ] \n",
      "Feature 41:\t[ -1.2830\t14359.9185\t515345\t3537.5036\t-4673.3554 ] \n",
      "Feature 42:\t[ 18.1480\t14363.0035\t515345\t3892.1248\t-4175.4127 ] \n",
      "Feature 43:\t[ -51.9590\t5420.2602\t515345\t1202.4915\t-4975.3818 ] \n",
      "Feature 44:\t[ 3.2327\t1472.5545\t515345\t1830.5447\t-1072.9555 ] \n",
      "Feature 45:\t[ -1.4883\t1726.0097\t515345\t746.7075\t-1021.2892 ] \n",
      "Feature 46:\t[ 6.3341\t3022.1841\t515345\t1198.6268\t-1329.9597 ] \n",
      "Feature 47:\t[ 78.7024\t221461.9080\t515345\t9059.7598\t-14861.6953 ] \n",
      "Feature 48:\t[ 142.6969\t68838.0495\t515345\t6967.6398\t-3992.6887 ] \n",
      "Feature 49:\t[ -86.5165\t43904.1429\t515345\t6172.3450\t-6642.3996 ] \n",
      "Feature 50:\t[ 25.2408\t14921.5056\t515345\t2067.2045\t-2344.5265 ] \n",
      "Feature 51:\t[ 6.3785\t8752.6638\t515345\t1426.8480\t-2270.8111 ] \n",
      "Feature 52:\t[ 28.2941\t5632.3939\t515345\t2460.4334\t-1746.4782 ] \n",
      "Feature 53:\t[ 12.7722\t4897.4592\t515345\t2394.6623\t-3188.1774 ] \n",
      "Feature 54:\t[ 1.7005\t6935.3352\t515345\t2900.5202\t-2199.7822 ] \n",
      "Feature 55:\t[ -10.2052\t3341.6020\t515345\t569.1358\t-1694.2603 ] \n",
      "Feature 56:\t[ 64.1013\t74981.3292\t515345\t6955.4147\t-5154.0244 ] \n",
      "Feature 57:\t[ 104.8221\t96805.1738\t515345\t12700.0143\t-5111.6017 ] \n",
      "Feature 58:\t[ -0.0265\t71168.3813\t515345\t13001.2590\t-4730.5991 ] \n",
      "Feature 59:\t[ 38.6780\t28585.3963\t515345\t5419.2769\t-3756.4908 ] \n",
      "Feature 60:\t[ -27.9900\t20759.9209\t515345\t5690.2917\t-2499.9547 ] \n",
      "Feature 61:\t[ 3.3017\t3533.9700\t515345\t1811.2287\t-1900.1048 ] \n",
      "Feature 62:\t[ 0.3071\t2418.8126\t515345\t973.0530\t-1396.7017 ] \n",
      "Feature 63:\t[ -0.4788\t1419.4376\t515344\t812.4243\t-600.0908 ] \n",
      "Feature 64:\t[ -138.2232\t94979.4675\t515345\t11048.1982\t-10345.8333 ] \n",
      "Feature 65:\t[ -0.6961\t49372.1819\t515345\t2877.7385\t-7375.9774 ] \n",
      "Feature 66:\t[ 0.2426\t16420.1058\t515345\t3447.4790\t-3896.2752 ] \n",
      "Feature 67:\t[ 3.1518\t9984.1961\t515345\t2055.0395\t-1199.0044 ] \n",
      "Feature 68:\t[ 27.6427\t13617.9976\t515345\t4779.8003\t-2564.7881 ] \n",
      "Feature 69:\t[ 31.8222\t11312.4363\t515345\t5286.8216\t-1904.9843 ] \n",
      "Feature 70:\t[ -0.8360\t1354.1806\t515345\t745.5034\t-974.7025 ] \n",
      "Feature 71:\t[ -8.9316\t63305.9587\t515345\t3958.0701\t-7057.7124 ] \n",
      "Feature 72:\t[ 4.8493\t52468.6767\t515345\t4741.1810\t-6953.3574 ] \n",
      "Feature 73:\t[ -27.3476\t26824.7465\t515345\t2124.1009\t-8400.6033 ] \n",
      "Feature 74:\t[ -11.9388\t4005.4837\t515344\t1639.9304\t-1812.8894 ] \n",
      "Feature 75:\t[ -21.5721\t4184.9960\t515345\t1278.3333\t-1387.5055 ] \n",
      "Feature 76:\t[ -5.5762\t694.7534\t515345\t741.0290\t-718.4211 ] \n",
      "Feature 77:\t[ -23.3043\t71926.3654\t515345\t10020.2832\t-9831.4539 ] \n",
      "Feature 78:\t[ 31.1131\t20781.0110\t515345\t3423.5954\t-2025.7782 ] \n",
      "Feature 79:\t[ -104.9748\t40456.3028\t515345\t5188.3273\t-8390.0354 ] \n",
      "Feature 80:\t[ 26.9624\t15338.3001\t515345\t3735.0256\t-4754.9372 ] \n",
      "Feature 81:\t[ 15.7554\t1030.3866\t515345\t840.9734\t-437.7220 ] \n",
      "Feature 82:\t[ -73.4615\t30841.9943\t515345\t4469.4549\t-4402.3764 ] \n",
      "Feature 83:\t[ 41.5424\t14939.8793\t515345\t3210.7017\t-1810.6892 ] \n",
      "Feature 84:\t[ 37.9341\t9034.6224\t515345\t1734.0797\t-3098.3503 ] \n",
      "Feature 85:\t[ 0.3158\t261.2026\t515345\t260.5449\t-341.7891 ] \n",
      "Feature 86:\t[ 17.6692\t13093.7454\t515345\t3662.0657\t-3168.9246 ] \n",
      "Feature 87:\t[ -26.3153\t30268.1135\t515345\t2833.6089\t-4319.9923 ] \n",
      "Feature 88:\t[ 4.4586\t178.1306\t515345\t463.4195\t-236.0393 ] \n",
      "Feature 89:\t[ 20.0351\t34431.8629\t515345\t7393.3984\t-7458.3781 ] \n",
      "Feature 90:\t[ 1.3291\t487.9052\t515345\t677.8996\t-381.4244 ] \n",
      "Label:\n",
      "Label :\t[ 1998.40\t119.49\t515345\t2011.00\t1922.00 ] \n",
      "Pearson Correlation test: \n",
      "Feature 15 and Feature 17 have r=0.809553568338\n",
      "Feature 15 and Feature 22 have r=0.846648609559\n",
      "Feature 17 and Feature 22 have r=0.859568622419\n",
      "Feature 19 and Feature 21 have r=0.865683998762\n",
      "***********************************************\n",
      "\n",
      "Feature Engineering\n",
      "\n",
      "***********************************************\n",
      "Features:\n",
      "Format:\t\t[ mean\tvariance\tnumNonzeors\tmax\tmin ]\n",
      "Feature 1:\t[ -0.1062\t0.1275\t463715\t0.7495\t-0.9488 ] \n",
      "Feature 2:\t[ 0.0016\t0.0628\t463715\t0.7635\t-0.8037 ] \n",
      "Feature 3:\t[ -0.0074\t0.0520\t463715\t0.7273\t-0.7982 ] \n",
      "Feature 4:\t[ -0.0303\t0.0456\t463715\t0.6977\t-0.7634 ] \n",
      "Feature 5:\t[ 0.0095\t0.0333\t463715\t0.6942\t-0.7803 ] \n",
      "Feature 6:\t[ 0.0018\t0.0277\t463715\t0.7386\t-0.6481 ] \n",
      "Feature 7:\t[ -0.0037\t0.0255\t463715\t0.6116\t-0.6758 ] \n",
      "Feature 8:\t[ -0.0038\t0.0242\t463715\t0.6248\t-0.7383 ] \n",
      "Feature 9:\t[ 0.0017\t0.0234\t463715\t0.6219\t-0.6840 ] \n",
      "Feature 10:\t[ -0.0009\t0.0209\t463715\t0.6631\t-0.5997 ] \n",
      "Label:\n",
      "Label :\t[ 1998.39\t119.68\t463715\t2011.00\t1922.00 ] \n",
      "Pearson Correlation test: \n",
      "***********************************************\n",
      "\n",
      "Classification\n",
      "\n",
      "***********************************************\n",
      "==============================================\n",
      "Classification: SVMWithSGD\n",
      "Regulization Type: None\n",
      "==============================================\n",
      "Convert class variable ...\n",
      "Training ...\n",
      "Training Complete: \n",
      "\n",
      "Error Rate: 0.0116017819097 \n",
      "\n",
      "SVMWithSGD Model: \n",
      "[0.253651410946,0.00467353548773,0.04644120218,0.053170365431,-0.028125983681,0.0125582868081,0.00252465539331,-0.00117612958613,-0.00339580330413,0.000951880596201]\n",
      "\n",
      "Total Run Time: 8.75472402573 seconds\n",
      "==============================================\n",
      "Classification: SVMWithSGD\n",
      "Regulization Type: l1\n",
      "==============================================\n",
      "Convert class variable ...\n",
      "Training ...\n",
      "Training Complete: \n",
      "\n",
      "Error Rate: 0.0116017819097 \n",
      "\n",
      "SVMWithSGD Model: \n",
      "[0.0,0.0,0.0,-0.0,-0.0,0.0,-0.0,-0.0,0.0,-0.0]\n",
      "\n",
      "Total Run Time: 13.3584039211 seconds\n",
      "==============================================\n",
      "Classification: SVMWithSGD\n",
      "Regulization Type: l2\n",
      "==============================================\n",
      "Convert class variable ...\n",
      "Training ...\n",
      "Training Complete: \n",
      "\n",
      "Error Rate: 0.0116017819097 \n",
      "\n",
      "SVMWithSGD Model: \n",
      "[0.117483777383,0.0142549042941,0.0216757049882,0.0278715259209,-0.00440176484907,0.0211622599332,0.00147057581957,-0.0105842928454,-0.00337119322027,0.00593793477638]\n",
      "\n",
      "Total Run Time: 7.56760907173 seconds\n",
      "==============================================\n",
      "Classification: LogisticRegressionWithLBFGS\n",
      "Regulization Type: l1\n",
      "==============================================\n",
      "Convert class variable ...\n",
      "Training ...\n",
      "Training Complete: \n",
      "\n",
      "Error Rate: 0.0116017819097 \n",
      "\n",
      "LogisticRegressionWithLBFGS Model: \n",
      "[0.244586609109,0.00543348519248,0.504810593154,-0.025347293168,-0.0309233118093,0.332784444712,-0.00829914162063,-0.00737622011326,0.000663350544674,-0.00436425168166]\n",
      "\n",
      "Total Run Time: 5.72720003128 seconds\n",
      "==============================================\n",
      "Classification: LogisticRegressionWithLBFGS\n",
      "Regulization Type: l2\n",
      "==============================================\n",
      "Convert class variable ...\n",
      "Training ...\n",
      "Training Complete: \n",
      "\n",
      "Error Rate: 0.0116017819097 "
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS,SVMWithSGD,LogisticRegressionWithSGD\n",
    "from pyspark.mllib.regression import LabeledPoint,LinearRegressionWithSGD,RidgeRegressionWithSGD,LassoWithSGD\n",
    "from pyspark.mllib.feature import StandardScaler,ChiSqSelector,Normalizer,PCA\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "from pyspark.mllib.stat import Statistics, MultivariateStatisticalSummary\n",
    "from pyspark.mllib.util import MLUtils\n",
    "import time\n",
    "\n",
    "def importRawData(sc, filePath):\n",
    "    '''\n",
    "    :param sc: Spark Context\n",
    "    :param filePath: path to data .csv file\n",
    "    :return: RDD of (LabeledPoint, index)\n",
    "    '''\n",
    "    rdd = sc.textFile(filePath)\n",
    "    return rdd.map(lambda line: line.split(\",\")) \\\n",
    "                .map(lambda array: LabeledPoint(int(array[0]),[float(i) for i in array[1:]])) \\\n",
    "                .zipWithIndex()\n",
    "\n",
    "def exploreData(rawData):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    x = rawData.map(lambda lp:lp[0].features)\n",
    "    y = rawData.map(lambda lp:[lp[0].label])\n",
    "    xTrait = Statistics.colStats(x)\n",
    "    print \"Features:\"\n",
    "    print \"Format:\\t\\t[ mean\\tvariance\\tnumNonzeors\\tmax\\tmin ]\"\n",
    "    for i in range(len(xTrait.mean())):\n",
    "        print \"Feature %s:\\t[ %.4f\\t%.4f\\t%d\\t%.4f\\t%.4f ] \" % (i+1, xTrait.mean()[i], xTrait.variance()[i],\\\n",
    "                                                    xTrait.numNonzeros()[i], xTrait.max()[i], xTrait.min()[i])\n",
    "    yTrait = Statistics.colStats(y)\n",
    "    print \"Label:\"\n",
    "    print \"Label :\\t[ %.2f\\t%.2f\\t%d\\t%.2f\\t%.2f ] \" % (yTrait.mean()[0], yTrait.variance()[0], \\\n",
    "                                            yTrait.numNonzeros()[0], yTrait.max()[0], yTrait.min()[0])\n",
    "    corr = Statistics.corr(x, method=\"pearson\")\n",
    "    print \"Pearson Correlation test: \"\n",
    "    for i, row in enumerate(corr):\n",
    "        for j, r in enumerate(row):\n",
    "            if (i<j and r>0.8):\n",
    "                print \"Feature %s and Feature %s have r=%s\" % (i, j, r)\n",
    "                \n",
    "def featureEngineering(rawData, zNorm = True, l2Norm = True, chiSq = False, topFeature = 10):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    #this partition is set by the suggestion of data set\n",
    "    #first 463715 used as trainning, last 51630 used as validation\n",
    "    tFeatures = rawData.filter(lambda lp:lp[1]<463715).map(lambda lp:lp[0].features)\n",
    "    vFeatures = rawData.filter(lambda lp:lp[1]>=463715).map(lambda lp:lp[0].features)\n",
    "    tLabel = rawData.filter(lambda lp:lp[1]<463715).map(lambda lp:lp[0].label)\n",
    "    vLabel = rawData.filter(lambda lp:lp[1]>=463715).map(lambda lp:lp[0].label)\n",
    "    if(zNorm):\n",
    "        zN = StandardScaler(withMean=True, withStd=True).fit(tFeatures)\n",
    "        tFeatures = zN.transform(tFeatures)\n",
    "        vFeatures = zN.transform(vFeatures)\n",
    "    if(l2Norm):\n",
    "        l2N = Normalizer()\n",
    "        tFeatures = l2N.transform(tFeatures)\n",
    "        vFeatures = l2N.transform(vFeatures)\n",
    "    if chiSq:\n",
    "        selector = ChiSqSelector(topFeature).fit(tFeatures)\n",
    "        tFeatures = selector.transform(tFeatures)\n",
    "        vFeatures = selector.transform(vFeatures)\n",
    "    else:\n",
    "        selector = PCA(topFeature).fit(tFeatures)\n",
    "        tFeatures = selector.transform(tFeatures)\n",
    "        vFeatures = selector.transform(vFeatures)\n",
    "    return (tLabel.zip(tFeatures).map(lambda lp:LabeledPoint(lp[0], lp[1])).cache(), \\\n",
    "            vLabel.zip(vFeatures).map(lambda lp:LabeledPoint(lp[0], lp[1])).cache())\n",
    "    \n",
    "\n",
    "def trainClassificationModel(sc, trainingData, validationData, modelClass, **kwargs):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    print \"==============================================\"\n",
    "    print \"Classification: %s\" % modelClass.__name__\n",
    "    if \"regType\" in kwargs:\n",
    "        print \"Regulization Type: %s\" % kwargs[\"regType\"]\n",
    "    print \"==============================================\"\n",
    "    startTime = time.time()\n",
    "    print \"Convert class variable ...\"\n",
    "    trainingData = trainingData \\\n",
    "                    .map(lambda lp:LabeledPoint(0, lp.features) if lp.label>=1965 else LabeledPoint(1, lp.features))\n",
    "    validationData = validationData \\\n",
    "                    .map(lambda lp:LabeledPoint(0, lp.features) if lp.label>=1965 else LabeledPoint(1, lp.features))\n",
    "    print \"Training ...\"\n",
    "    model = modelClass.train(trainingData, **kwargs)\n",
    "    print \"Training Complete: \"\n",
    "    validationsResult = validationData.map(lambda lp:(lp.label, model.predict(lp.features)))\n",
    "    #use error rate as measurement\n",
    "    modelErr = validationsResult.filter(lambda (actual, predict):actual!=predict).count() / float(validationsResult.count())\n",
    "    print \"\\nError Rate: %s \" % modelErr\n",
    "    print \"\\n%s Model: \" % modelClass.__name__\n",
    "    print model.weights\n",
    "    print(\"\\nTotal Run Time: %s seconds\" % (time.time() - startTime))\n",
    "        \n",
    "if __name__==\"__main__\":\n",
    "    filePath = \"file:///ipython/YearPredictionMSD.txt\"\n",
    "    sc = SparkContext(appName=\"MainContext\")\n",
    "    rawData = importRawData(sc, filePath).cache()\n",
    "    try:\n",
    "        print \"File Path: %s\" % filePath\n",
    "        print \"***********************************************\"\n",
    "        print \"\\nData Explore\\n\"\n",
    "        print \"***********************************************\"\n",
    "        exploreData(rawData)\n",
    "        print \"***********************************************\"\n",
    "        print \"\\nFeature Engineering\\n\"\n",
    "        print \"***********************************************\"\n",
    "        trainingData, validationData = featureEngineering(rawData, topFeature=10)\n",
    "        exploreData(trainingData.zipWithIndex())\n",
    "        print \"***********************************************\"\n",
    "        print \"\\nClassification\\n\"\n",
    "        print \"***********************************************\"\n",
    "        #tuple array contains (ModelClass, [regulization types])\n",
    "        classificationModels = [(SVMWithSGD, [None, \"l1\", \"l2\"]), \n",
    "                                (LogisticRegressionWithLBFGS, [\"l1\", \"l2\"]), \n",
    "                                (LogisticRegressionWithSGD, [None, \"l1\", \"l2\"])]\n",
    "        for classificationModel, regTypes in classificationModels:\n",
    "            for regType_ in regTypes:\n",
    "                trainClassificationModel(sc, trainingData, validationData, classificationModel, iterations=200, intercept=True, regType=regType_)\n",
    "            if len(regTypes)==0:\n",
    "                trainClassificationModel(sc, trainingData, validationData, classificationModel, intercept=True, iterations=100)\n",
    "    except Exception:\n",
    "        raise\n",
    "    finally:\n",
    "        sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}